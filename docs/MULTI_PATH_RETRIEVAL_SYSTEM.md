# YoutuGraphRAG å¤šè·¯æ··åˆæ£€ç´¢ç³»ç»Ÿè¯¦è§£

## ğŸ“‹ ç›®å½•

- [1. ç³»ç»Ÿæ¦‚è¿°](#1-ç³»ç»Ÿæ¦‚è¿°)
- [2. æ•´ä½“æ¶æ„ä¸å…¥å£](#2-æ•´ä½“æ¶æ„ä¸å…¥å£)
- [3. æ£€ç´¢ç­–ç•¥é€‰æ‹©æœºåˆ¶](#3-æ£€ç´¢ç­–ç•¥é€‰æ‹©æœºåˆ¶)
- [4. è·¯å¾„1ï¼šGraph-Firstæ™ºèƒ½æ£€ç´¢](#4-è·¯å¾„1graph-firstæ™ºèƒ½æ£€ç´¢)
- [5. è·¯å¾„2ï¼šå¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢](#5-è·¯å¾„2å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢)
- [6. è·¯å¾„3ï¼šè¯­ä¹‰å‘é‡æ£€ç´¢](#6-è·¯å¾„3è¯­ä¹‰å‘é‡æ£€ç´¢)
- [7. ç»“æœèåˆä¸é™çº§ç­–ç•¥](#7-ç»“æœèåˆä¸é™çº§ç­–ç•¥)
- [8. å¤šå­é—®é¢˜å¤„ç†æœºåˆ¶](#8-å¤šå­é—®é¢˜å¤„ç†æœºåˆ¶)
- [9. åŠ¨æ€ç´¢å¼•ä¸ç¼“å­˜æœºåˆ¶](#9-åŠ¨æ€ç´¢å¼•ä¸ç¼“å­˜æœºåˆ¶)
- [10. å®é™…è¿è¡Œç¤ºä¾‹](#10-å®é™…è¿è¡Œç¤ºä¾‹)
- [11. æ ¸å¿ƒä¼˜åŠ¿åˆ†æ](#11-æ ¸å¿ƒä¼˜åŠ¿åˆ†æ)

## 1. ç³»ç»Ÿæ¦‚è¿°

YoutuGraphRAG é‡‡ç”¨å¤šè·¯æ··åˆæ£€ç´¢æ¶æ„ï¼Œé€šè¿‡æ™ºèƒ½ç­–ç•¥é€‰æ‹©å’Œå¤šç§æ£€ç´¢è·¯å¾„ä»å¤šä¸ªè§’åº¦ç†è§£å’Œå¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œç¡®ä¿æ£€ç´¢çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†å›¾ç»“æ„éå†ã€å‘é‡ç›¸ä¼¼åº¦æœç´¢å’Œè¯­ä¹‰ç†è§£ç­‰å¤šç§æŠ€æœ¯ï¼Œå½¢æˆä¸€ä¸ªäº’è¡¥çš„æ£€ç´¢ç½‘ç»œã€‚

### 1.1 æ ¸å¿ƒç‰¹æ€§

- **Graph-First ç­–ç•¥**ï¼šä¼˜å…ˆåˆ©ç”¨å›¾è°±ç»“æ„è¿›è¡Œç²¾ç¡®æ¨ç†ï¼Œä¸“ä¸ºä½ç½®æŸ¥è¯¢ä¼˜åŒ–
- **æ™ºèƒ½ç­–ç•¥é€‰æ‹©**ï¼šæ ¹æ®æŸ¥è¯¢ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ£€ç´¢ç­–ç•¥
- **å¤šè·¯å¾„å¹¶è¡Œ**ï¼šå¤šç§æ£€ç´¢è·¯å¾„åŒæ—¶æ‰§è¡Œï¼Œä¼˜åŠ¿äº’è¡¥
- **æ™ºèƒ½èåˆ**ï¼šæ ¹æ®æŸ¥è¯¢ç‰¹å¾åŠ¨æ€è°ƒæ•´å„è·¯å¾„æƒé‡
- **é™çº§å›é€€**ï¼šå½“ä¸»è¦ç­–ç•¥å¤±è´¥æ—¶è‡ªåŠ¨å›é€€åˆ°å¤‡ç”¨ç­–ç•¥
- **åŠ¨æ€ç´¢å¼•**ï¼šè¿è¡Œæ—¶æ„å»ºçš„å†…å­˜ç´¢å¼•ï¼Œæ”¯æŒçµæ´»çš„æ•°æ®æ›´æ–°

### 1.2 æ£€ç´¢ç­–ç•¥å¯¹æ¯”

| æ£€ç´¢ç­–ç•¥ | æŠ€æœ¯æ–¹æ³• | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | å›é€€æœºåˆ¶ |
|---------|---------|----------|------|---------|
| **Graph-First** | å›¾éå† + æ™ºèƒ½åˆ†æ | ä½ç½®æŸ¥è¯¢ | ç²¾ç¡®ã€å¿«é€Ÿ | å›é€€åˆ°å¤šè·¯å¾„æ£€ç´¢ |
| **å¤šè·¯å¾„å¹¶è¡Œ** | FAISS + å…³é”®è¯ + ä¸‰å…ƒç»„ | é€šç”¨æŸ¥è¯¢ | å…¨é¢ã€è¯­ä¹‰ç†è§£ | é™æƒå¤„ç† |
| **è¯­ä¹‰æ£€ç´¢** | Chunkå‘é‡æœç´¢ | æ¨¡ç³ŠæŸ¥è¯¢ | ç›´æ¥åŒ¹é… | è§„åˆ™åŒ¹é…åå¤‡ |

## 2. æ•´ä½“æ¶æ„ä¸å…¥å£

### 2.1 ç³»ç»Ÿå…¥å£ç‚¹

**ä¸»è¦å…¥å£**ï¼š`KTRetriever.retrieve(question: str) -> Dict`

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py
def retrieve(self, question: str) -> Dict:
    """ä¸»æ£€ç´¢å…¥å£ï¼Œæ™ºèƒ½é€‰æ‹©æ£€ç´¢ç­–ç•¥"""
    # 1. æŸ¥è¯¢å‘é‡åŒ–
    question_embed = self._get_query_embedding(question)
    
    # 2. ç­–ç•¥é€‰æ‹©ï¼šæ£€æŸ¥æ˜¯å¦è§¦å‘Graph-First
    if self._should_use_graph_first(question):
        return self._execute_graph_first_strategy(question, question_embed)
    
    # 3. å›é€€åˆ°å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢
    return self._parallel_dual_path_retrieval(question_embed, question)
```

**å­é—®é¢˜å…¥å£**ï¼š`KTRetriever.process_subquestions_parallel(sub_questions: List[Dict]) -> Tuple[Dict, float]`

### 2.2 æ•´ä½“æ¶æ„æµç¨‹å›¾

```mermaid
graph TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B[æŸ¥è¯¢å‘é‡åŒ–]
    B --> C{ç­–ç•¥é€‰æ‹©å™¨}
    
    C -->|ä½ç½®æŸ¥è¯¢| D[Graph-Firstç­–ç•¥]
    C -->|é€šç”¨æŸ¥è¯¢| E[å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢]
    
    D --> D1[æ™ºèƒ½ä½ç½®åˆ†æ]
    D1 --> D2[åŠ¨æ€æ£€ç´¢ç­–ç•¥ç”Ÿæˆ]
    D2 --> D3[å›¾éå†æ‰§è¡Œ]
    D3 --> D4{æ‰¾åˆ°ç»“æœ?}
    D4 -->|æ˜¯| F[ç»“æœèåˆ]
    D4 -->|å¦| G[å›é€€åˆ°å¤šè·¯å¾„æ£€ç´¢]
    
    E --> E1[FAISSèŠ‚ç‚¹æ£€ç´¢]
    E --> E2[å…³é”®è¯æ£€ç´¢]
    E --> E3[ä¸‰å…ƒç»„æ£€ç´¢]
    E --> E4[Chunkè¯­ä¹‰æ£€ç´¢]
    
    E1 --> F
    E2 --> F
    E3 --> F
    E4 --> F
    G --> F
    
    F --> H[æƒé‡åˆ†é…ä¸æ’åº]
    H --> I[Chunk IDæå–]
    I --> J[æœ€ç»ˆç»“æœè¾“å‡º]
    
    style D fill:#e1f5fe
    style E fill:#f3e5f5
    style F fill:#e8f5e8
    style G fill:#fff3e0
```

### 2.3 æ ¸å¿ƒç»„ä»¶å…³ç³»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KTRetriever ä¸»ç±»                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Graph-First   â”‚  â”‚   å¤šè·¯å¾„æ£€ç´¢     â”‚  â”‚   ç»“æœèåˆå™¨     â”‚ â”‚
â”‚  â”‚   ç­–ç•¥å¼•æ“      â”‚  â”‚   å¹¶è¡Œæ‰§è¡Œå™¨     â”‚  â”‚   æ™ºèƒ½æ’åºå™¨     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                     â”‚                     â”‚       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   ä½ç½®åˆ†æå™¨     â”‚  â”‚   FAISSæ£€ç´¢å™¨   â”‚  â”‚   æƒé‡åˆ†é…å™¨     â”‚ â”‚
â”‚  â”‚   åŠ¨æ€ç­–ç•¥ç”Ÿæˆ   â”‚  â”‚   å…³é”®è¯æ£€ç´¢å™¨   â”‚  â”‚   å»é‡åˆå¹¶å™¨     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3. æ£€ç´¢ç­–ç•¥é€‰æ‹©æœºåˆ¶

### 3.1 ç­–ç•¥é€‰æ‹©é€»è¾‘

ç³»ç»Ÿé‡‡ç”¨æ™ºèƒ½ç­–ç•¥é€‰æ‹©æœºåˆ¶ï¼Œæ ¹æ®æŸ¥è¯¢ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ£€ç´¢ç­–ç•¥ï¼š

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬601è¡Œ)
def retrieve(self, question: str) -> Dict:
    # æ£€æŸ¥æ˜¯å¦è§¦å‘Graph-Firstç­–ç•¥
    import re
    if re.search(r'[AB]æ ‹.*\d+å±‚|[AB]æ ‹.*åœ°ä¸‹ä¸€å±‚|[AB]æ ‹.*B1|Aæ ‹.*B1|Bæ ‹.*B1', question):
        logger.info(f"[GraphFirst] æ£€æµ‹åˆ°ä½ç½®æŸ¥è¯¢ï¼Œè§¦å‘Graph-Firstç­–ç•¥: {question}")
        # æ‰§è¡ŒGraph-Firstç­–ç•¥
        return self._execute_graph_first_strategy(question, question_embed)
    else:
        # å›é€€åˆ°å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢
        return self._parallel_dual_path_retrieval(question_embed, question)
```

### 3.2 è§¦å‘æ¡ä»¶

**Graph-Firstç­–ç•¥è§¦å‘æ¡ä»¶**ï¼š
- æŸ¥è¯¢åŒ…å«å»ºç­‘æ ‡è¯†ï¼š`Aæ ‹` æˆ– `Bæ ‹`
- æŸ¥è¯¢åŒ…å«æ¥¼å±‚ä¿¡æ¯ï¼š`3å±‚`ã€`åœ°ä¸‹ä¸€å±‚`ã€`B1`ç­‰
- æ­£åˆ™è¡¨è¾¾å¼ï¼š`r'[AB]æ ‹.*\d+å±‚|[AB]æ ‹.*åœ°ä¸‹ä¸€å±‚|[AB]æ ‹.*B1|Aæ ‹.*B1|Bæ ‹.*B1'`

**å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢è§¦å‘æ¡ä»¶**ï¼š
- ä¸æ»¡è¶³Graph-Firstè§¦å‘æ¡ä»¶çš„æ‰€æœ‰å…¶ä»–æŸ¥è¯¢
- Graph-Firstç­–ç•¥æ‰§è¡Œå¤±è´¥æ—¶çš„å›é€€

### 3.3 ç­–ç•¥é€‰æ‹©æµç¨‹å›¾

```mermaid
flowchart TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B[æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…]
    B --> C{åŒ…å«ä½ç½®ä¿¡æ¯?}
    
    C -->|æ˜¯| D[Graph-Firstç­–ç•¥]
    C -->|å¦| E[å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢]
    
    D --> D1[æ™ºèƒ½ä½ç½®åˆ†æ]
    D1 --> D2[åŠ¨æ€æ£€ç´¢ç­–ç•¥ç”Ÿæˆ]
    D2 --> D3[å›¾éå†æ‰§è¡Œ]
    D3 --> D4{æ‰¾åˆ°ç»“æœ?}
    
    D4 -->|æ˜¯| F[è¿”å›Graph-Firstç»“æœ]
    D4 -->|å¦| G[å›é€€åˆ°å¤šè·¯å¾„æ£€ç´¢]
    
    E --> E1[FAISS + å…³é”®è¯ + ä¸‰å…ƒç»„ + Chunk]
    E1 --> H[è¿”å›å¤šè·¯å¾„ç»“æœ]
    
    G --> E1
    
    style D fill:#e1f5fe
    style E fill:#f3e5f5
    style G fill:#fff3e0
```

## 4. è·¯å¾„1ï¼šGraph-Firstæ™ºèƒ½æ£€ç´¢

### 4.1 å·¥ä½œåŸç†

Graph-Firstç­–ç•¥æ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒåˆ›æ–°ï¼Œä¸“ä¸ºä½ç½®æŸ¥è¯¢ä¼˜åŒ–ã€‚å®ƒé€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°ï¼š

1. **æ™ºèƒ½ä½ç½®åˆ†æ**ï¼šåŠ¨æ€åˆ†æé—®é¢˜ä¸­çš„ä½ç½®ä¿¡æ¯
2. **å›¾è°±æ¨¡å¼åˆ†æ**ï¼šåˆ†æå›¾è°±ä¸­çš„ä½ç½®èŠ‚ç‚¹æ¨¡å¼
3. **åŠ¨æ€ç­–ç•¥ç”Ÿæˆ**ï¼šåŸºäºé—®é¢˜å†…å®¹å’Œå›¾è°±ç»“æ„ç”Ÿæˆæ£€ç´¢ç­–ç•¥
4. **å›¾éå†æ‰§è¡Œ**ï¼šæ‰§è¡ŒåŠ¨æ€æ£€ç´¢ç­–ç•¥
5. **ç»“æœè¿‡æ»¤æ’åº**ï¼šæ ¹æ®å»ºç­‘ä¿¡æ¯è¿‡æ»¤å’Œæ’åºç»“æœ

### 4.2 æ ¸å¿ƒä»£ç å®ç°

#### 4.2.1 Graph-Firstç­–ç•¥ä¸»æµç¨‹

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬1588è¡Œ)
def _path_strategy(self, question: str, question_embed: torch.Tensor = None) -> List[Tuple[str, str, str, float]]:
    """Intelligent Graph-first path strategy: dynamically analyze question and graph structure."""
    try:
        # 1) æ™ºèƒ½åˆ†æé—®é¢˜ä¸­çš„ä½ç½®ä¿¡æ¯
        location_info = self._analyze_location_in_question(question)
        logger.info(f"[GraphFirst] æ™ºèƒ½åˆ†æä½ç½®ä¿¡æ¯: {location_info}")
        
        # 2) æ™ºèƒ½åˆ†æå›¾è°±ä¸­çš„ä½ç½®èŠ‚ç‚¹æ¨¡å¼
        location_patterns = self._analyze_location_patterns_in_graph()
        logger.info(f"[GraphFirst] å›¾è°±ä½ç½®æ¨¡å¼: {location_patterns}")
        
        # 3) åŸºäºé—®é¢˜å†…å®¹å’Œå›¾è°±ç»“æ„åŠ¨æ€ç”Ÿæˆæ£€ç´¢ç­–ç•¥
        search_strategy = self._generate_dynamic_search_strategy(question, location_info, location_patterns)
        logger.info(f"[GraphFirst] åŠ¨æ€æ£€ç´¢ç­–ç•¥: {search_strategy}")
        
        # 4) æ‰§è¡ŒåŠ¨æ€æ£€ç´¢ç­–ç•¥
        triples = self._execute_dynamic_search(search_strategy, question_embed)
        
        # 5) æ ¹æ®é—®é¢˜ä¸­çš„å»ºç­‘ä¿¡æ¯è¿‡æ»¤å’Œæ’åºç»“æœ
        target_buildings = location_info.get('buildings', [])
        triples = self._rank_triples_by_relevance(triples, question_embed, target_buildings)
        
        return triples
    except Exception as e:
        logger.error(f"Intelligent path strategy failed: {e}")
        return []
```

#### 4.2.2 æ™ºèƒ½ä½ç½®åˆ†æ

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬1146è¡Œ)
def _analyze_location_in_question(self, question: str) -> dict:
    """æ™ºèƒ½åˆ†æé—®é¢˜ä¸­çš„ä½ç½®ä¿¡æ¯"""
    location_info = {
        'buildings': [],
        'floors': [],
        'rooms': [],
        'locations': [],
        'keywords': []
    }
    
    # åˆ†ææ‰€æœ‰èŠ‚ç‚¹æ–‡æœ¬ï¼Œæå–ä½ç½®æ¨¡å¼
    all_node_texts = []
    for node_id, node_data in self.graph.nodes(data=True):
        node_text = self._get_node_text(node_id)
        if node_text and not node_text.startswith('[Error'):
            all_node_texts.append(node_text)
    
    # ä»é—®é¢˜ä¸­æå–å¯èƒ½çš„ä½ç½®å…³é”®è¯
    question_lower = question.lower()
    
    # æ™ºèƒ½è¯†åˆ«å»ºç­‘æ ‡è¯†
    building_patterns = []
    for text in all_node_texts:
        if 'æ ‹' in text and len(text) < 20:  # çŸ­æ–‡æœ¬æ›´å¯èƒ½æ˜¯å»ºç­‘æ ‡è¯†
            building_match = re.search(r'([A-Za-z\u4e00-\u9fff]+)æ ‹', text)
            if building_match:
                building_patterns.append(building_match.group(1))
    
    # ä»é—®é¢˜ä¸­åŒ¹é…å»ºç­‘
    for building in building_patterns:
        if building in question:
            location_info['buildings'].append(building)
    
    # æ™ºèƒ½è¯†åˆ«æ¥¼å±‚ä¿¡æ¯
    floor_patterns = []
    for text in all_node_texts:
        if 'å±‚' in text and len(text) < 20:
            floor_match = re.search(r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)å±‚|(\d+)å±‚|(\d+)F', text)
            if floor_match:
                floor_patterns.append(floor_match.group(0))
    
    # ä»é—®é¢˜ä¸­åŒ¹é…æ¥¼å±‚
    for floor in floor_patterns:
        if floor in question:
            location_info['floors'].append(floor)
    
    return location_info
```

#### 4.2.3 åŠ¨æ€æ£€ç´¢ç­–ç•¥ç”Ÿæˆ

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬1262è¡Œ)
def _generate_dynamic_search_strategy(self, question: str, location_info: dict, location_patterns: dict) -> dict:
    """åŸºäºé—®é¢˜å†…å®¹å’Œå›¾è°±ç»“æ„åŠ¨æ€ç”Ÿæˆæ£€ç´¢ç­–ç•¥"""
    strategy = {
        'search_type': 'building_floor_equipment',
        'target_nodes': [],
        'search_paths': ['floor -> room -> equipment', 'floor -> equipment'],
        'priority_relations': ['located_in', 'part_of', 'has_attribute', 'belongs_to_system'],
        'equipment_filters': ['è®¾å¤‡', 'å†·æœº', 'æ°´æ³µ', 'æ³µ', 'é…ç”µ', 'ç©ºè°ƒ', 'æœºç»„', 'æŸœ', 'ç®±', 'æœ«ç«¯', 'èºæ†', 'ç¦»å¿ƒ', 'é…ç”µæŸœ', 'ç©ºè°ƒç®±', 'å˜é£é‡', 'æ¶ˆé˜²', 'æ°´æ³µæˆ¿']
    }
    
    buildings = location_info.get('buildings', [])
    floors = location_info.get('floors', [])
    
    if not buildings or not floors:
        return strategy
    
    building = buildings[0]
    floor = floors[0]
    
    # 1. æŸ¥æ‰¾ç›¸å…³çš„LOCèŠ‚ç‚¹
    for node_id, node_text in location_patterns['location_nodes']:
        # æ„å»ºæ¥¼å±‚ä»£ç è¿›è¡Œç²¾ç¡®åŒ¹é…
        floor_code = self._convert_floor_to_code(floor)
        if f"LOC-{building}-{floor_code}" in node_text:
            strategy['target_nodes'].append(node_id)
            logger.info(f"[GraphFirst] æ·»åŠ LOCèŠ‚ç‚¹: {node_id} -> {node_text}")
    
    # 2. å¢å¼ºåŒ¹é…ï¼šæŸ¥æ‰¾ç‰¹å®šå»ºç­‘æ¥¼å±‚çš„ç›¸å…³èŠ‚ç‚¹
    for node_id, node_data in self.graph.nodes(data=True):
        node_text = self._get_node_text(node_id)
        if node_text and not node_text.startswith('[Error'):
            # åŠ¨æ€æ„å»ºåŒ¹é…æ¡ä»¶
            building_floor_match = False
            
            # æ„å»ºæ¥¼å±‚ä»£ç ï¼ˆå°†"åœ°ä¸‹ä¸€å±‚"è½¬æ¢ä¸º"B1"ç­‰ï¼‰
            floor_code = self._convert_floor_to_code(floor)
            
            # ä½¿ç”¨f-stringæ„å»ºåŒ¹é…æ¨¡å¼
            patterns_to_check = [
                f"{building}æ ‹{floor}",  # å¦‚ï¼šAæ ‹3å±‚
                f"{building}æ ‹{floor_code}",  # å¦‚ï¼šAæ ‹03
                f"LOC-{building}-{floor_code}",  # å¦‚ï¼šLOC-A-03
                f"location_id: LOC-{building}-{floor_code}"  # å¦‚ï¼šlocation_id: LOC-A-03
            ]
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•æ¨¡å¼
            building_floor_match = any(node_text.startswith(pattern) for pattern in patterns_to_check)
            
            if building_floor_match:
                strategy['target_nodes'].append(node_id)
                logger.info(f"[GraphFirst] å¢å¼ºåŒ¹é…èŠ‚ç‚¹: {node_id} -> {node_text}")
    
    return strategy
```

#### 4.2.4 å›¾éå†æ‰§è¡Œ

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬1395è¡Œ)
def _execute_dynamic_search(self, strategy: dict, question_embed: torch.Tensor) -> List[Tuple[str, str, str, float]]:
    """æ‰§è¡ŒåŠ¨æ€æ£€ç´¢ç­–ç•¥"""
    triples = []
    
    if strategy['search_type'] == 'building_floor_equipment':
        # å»ºç­‘æ¥¼å±‚è®¾å¤‡æœç´¢
        triples.extend(self._search_building_floor_equipment(strategy))
    elif strategy['search_type'] == 'building_equipment':
        # å»ºç­‘è®¾å¤‡æœç´¢
        triples.extend(self._search_building_equipment(strategy))
    elif strategy['search_type'] == 'room_equipment':
        # æˆ¿é—´è®¾å¤‡æœç´¢
        triples.extend(self._search_room_equipment(strategy))
    else:
        # é€šç”¨è®¾å¤‡æœç´¢
        triples.extend(self._search_general_equipment(strategy))
    
    # æŒ‰ç›¸å…³æ€§æ’åº
    triples = self._rank_triples_by_relevance(triples, question_embed)
    
    return triples[:self.top_k]

def _search_building_floor_equipment(self, strategy: dict) -> List[Tuple[str, str, str, float]]:
    """æœç´¢å»ºç­‘æ¥¼å±‚çš„è®¾å¤‡"""
    triples = []
    
    for target_node in strategy['target_nodes']:
        # ç›´æ¥æŸ¥æ‰¾è¯¥æ¥¼å±‚çš„è®¾å¤‡
        for u, v, data in self.graph.in_edges(target_node, data=True):
            if data.get('relation') == 'located_in':
                u_text = self._get_node_text(u)
                if any(keyword in u_text for keyword in strategy['equipment_filters']):
                    triples.append((u, 'located_in', v, 0.95))
        
        # æŸ¥æ‰¾è¯¥æ¥¼å±‚çš„æˆ¿é—´ï¼Œå†æŸ¥æ‰¾æˆ¿é—´å†…çš„è®¾å¤‡
        for u, v, data in self.graph.in_edges(target_node, data=True):
            if data.get('relation') == 'part_of':
                # uæ˜¯æˆ¿é—´ï¼ŒæŸ¥æ‰¾æˆ¿é—´å†…çš„è®¾å¤‡
                for a, b, d2 in self.graph.in_edges(u, data=True):
                    if d2.get('relation') == 'located_in':
                        a_text = self._get_node_text(a)
                        if any(keyword in a_text for keyword in strategy['equipment_filters']):
                            triples.append((a, 'located_in', b, 0.90))
        
        # å¦‚æœå½“å‰èŠ‚ç‚¹æ˜¯attributeèŠ‚ç‚¹ï¼ŒæŸ¥æ‰¾å¯¹åº”çš„entityèŠ‚ç‚¹
        node_data = self.graph.nodes[target_node]
        if node_data.get('label') == 'attribute':
            # æŸ¥æ‰¾å…·æœ‰ç›¸åŒåç§°çš„entityèŠ‚ç‚¹
            target_name = node_data.get('properties', {}).get('name', '')
            logger.info(f"[GraphFirst] å¤„ç†attributeèŠ‚ç‚¹: {target_node} -> {target_name}")
            # ä»attributeåç§°ä¸­æå–å®é™…çš„locationåç§°
            if target_name.startswith('location_id: '):
                actual_name = target_name.replace('location_id: ', '')
                logger.info(f"[GraphFirst] æå–çš„locationåç§°: {actual_name}")
                for node_id, node_data2 in self.graph.nodes(data=True):
                    if (node_data2.get('label') == 'entity' and 
                        node_data2.get('properties', {}).get('name') == actual_name):
                        logger.info(f"[GraphFirst] æ‰¾åˆ°å¯¹åº”çš„entityèŠ‚ç‚¹: {node_id}")
                        # æŸ¥æ‰¾entityèŠ‚ç‚¹çš„è®¾å¤‡
                        for u, v, data in self.graph.in_edges(node_id, data=True):
                            if data.get('relation') == 'located_in':
                                u_text = self._get_node_text(u)
                                logger.info(f"[GraphFirst] æ£€æŸ¥è®¾å¤‡: {u} -> {u_text}")
                                if any(keyword in u_text for keyword in strategy['equipment_filters']):
                                    logger.info(f"[GraphFirst] åŒ¹é…è®¾å¤‡: {u} -> {u_text}")
                                    triples.append((u, 'located_in', v, 0.95))
    
    return triples
```

### 4.3 Graph-Firstç­–ç•¥æµç¨‹å›¾

```mermaid
flowchart TD
    A[ä½ç½®æŸ¥è¯¢] --> B[æ™ºèƒ½ä½ç½®åˆ†æ]
    B --> C[å›¾è°±æ¨¡å¼åˆ†æ]
    C --> D[åŠ¨æ€ç­–ç•¥ç”Ÿæˆ]
    D --> E[å›¾éå†æ‰§è¡Œ]
    E --> F[ç»“æœè¿‡æ»¤æ’åº]
    F --> G{æ‰¾åˆ°ç»“æœ?}
    
    G -->|æ˜¯| H[è¿”å›ä¸‰å…ƒç»„]
    G -->|å¦| I[å›é€€åˆ°å¤šè·¯å¾„æ£€ç´¢]
    
    B --> B1[æå–å»ºç­‘ä¿¡æ¯]
    B --> B2[æå–æ¥¼å±‚ä¿¡æ¯]
    B --> B3[æå–ä½ç½®ç¼–ç ]
    
    C --> C1[åˆ†æå»ºç­‘èŠ‚ç‚¹]
    C --> C2[åˆ†ææ¥¼å±‚èŠ‚ç‚¹]
    C --> C3[åˆ†æä½ç½®èŠ‚ç‚¹]
    
    D --> D1[ç”Ÿæˆç›®æ ‡èŠ‚ç‚¹åˆ—è¡¨]
    D --> D2[è®¾ç½®è®¾å¤‡è¿‡æ»¤å™¨]
    D --> D3[å®šä¹‰æœç´¢è·¯å¾„]
    
    E --> E1[ç›´æ¥è®¾å¤‡æœç´¢]
    E --> E2[æˆ¿é—´è®¾å¤‡æœç´¢]
    E --> E3[å±æ€§èŠ‚ç‚¹å¤„ç†]
    
    style A fill:#e1f5fe
    style H fill:#e8f5e8
    style I fill:#fff3e0
```

## 5. è·¯å¾„2ï¼šå¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢

### 5.1 å·¥ä½œåŸç†

å½“Graph-Firstç­–ç•¥ä¸é€‚ç”¨æˆ–å¤±è´¥æ—¶ï¼Œç³»ç»Ÿå›é€€åˆ°å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢ã€‚è¯¥ç­–ç•¥é€šè¿‡ä»¥ä¸‹è·¯å¾„åŒæ—¶æ‰§è¡Œï¼š

1. **FAISSèŠ‚ç‚¹æ£€ç´¢**ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦çš„èŠ‚ç‚¹æœç´¢
2. **å…³é”®è¯æ£€ç´¢**ï¼šåŸºäºå…³é”®è¯åŒ¹é…çš„èŠ‚ç‚¹æœç´¢
3. **ä¸‰å…ƒç»„æ£€ç´¢**ï¼šåŸºäºä¸‰å…ƒç»„å‘é‡ç›¸ä¼¼åº¦çš„æœç´¢
4. **Chunkè¯­ä¹‰æ£€ç´¢**ï¼šåŸºäºæ–‡æ¡£å—çš„è¯­ä¹‰æœç´¢

### 5.2 æ ¸å¿ƒä»£ç å®ç°

#### 5.2.1 å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢ä¸»æµç¨‹

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬960è¡Œ)
def _parallel_dual_path_retrieval(self, question_embed: torch.Tensor, question: str) -> Dict:
    """å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢ä¸»æµç¨‹"""
    all_chunk_ids = set()
    start_time = time.time()
    
    max_workers = 4
    if self.config:
        max_workers = self.config.retrieval.faiss.max_workers
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # å¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªä¸»è¦è·¯å¾„
        path1_future = executor.submit(self._node_relation_retrieval, question_embed, question)
        path2_future = executor.submit(self._triple_only_retrieval, question_embed)
        
        # æ”¶é›†ç»“æœ
        path1_results = path1_future.result()
        path2_results = path2_future.result()

    # æå–chunk_ids
    path1_chunk_ids = self._extract_chunk_ids_from_nodes(path1_results['top_nodes'])
    path2_chunk_ids = self._extract_chunk_ids_from_triple_nodes(path2_results['scored_triples'])
    
    path3_chunk_ids = set()
    if 'chunk_results' in path1_results and path1_results['chunk_results']:
        path3_chunk_ids = set(path1_results['chunk_results'].get('chunk_ids', []))
    
    # åˆå¹¶æ‰€æœ‰chunk_ids
    all_chunk_ids.update(path1_chunk_ids)
    all_chunk_ids.update(path2_chunk_ids)
    all_chunk_ids.update(path3_chunk_ids) 
    
    limited_chunk_ids = list(all_chunk_ids)[:self.top_k]
    
    return {
        "path1_results": path1_results,
        "path2_results": path2_results,
        "chunk_ids": limited_chunk_ids 
    }
```

#### 5.2.2 èŠ‚ç‚¹å…³ç³»æ£€ç´¢ï¼ˆPath1ï¼‰

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬1616è¡Œ)
def _node_relation_retrieval(self, question_embed: torch.Tensor, question: str = "") -> Dict:
    """èŠ‚ç‚¹å…³ç³»æ£€ç´¢ï¼ŒåŒ…å«FAISSã€å…³é”®è¯ã€ä¸‰å…ƒç»„å’ŒChunkæ£€ç´¢"""
    overall_start = time.time()

    max_workers = 4
    if self.config:
        max_workers = self.config.retrieval.faiss.max_workers
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        q_embed = self.faiss_retriever.transform_vector(question_embed)
        search_k = min(self.top_k * 3, 50)
        
        # å¹¶è¡Œæ‰§è¡Œå¤šç§æ£€ç´¢ç­–ç•¥
        future_faiss_nodes = executor.submit(
            self._execute_faiss_node_search,
            q_embed.cpu().numpy(),
            search_k
        )

        future_keywords = future_keyword_nodes = None
        if question:
            future_keywords = executor.submit(
                self._extract_query_keywords,
                question
            )
            future_keyword_nodes = executor.submit(
                self._get_keyword_based_nodes,
                future_keywords
            )

        future_faiss_relations = executor.submit(
            self._execute_faiss_relation_search,
            q_embed.cpu().numpy()
        )

        future_chunk_retrieval = executor.submit(
            self._chunk_embedding_retrieval,
            question_embed,
            self.top_k
        )

        # æ”¶é›†ç»“æœ
        faiss_candidate_nodes = future_faiss_nodes.result()
        # ... å…¶ä»–ç»“æœæ”¶é›†é€»è¾‘
        
    return {
        "top_nodes": top_nodes,
        "one_hop_triples": one_hop_triples,
        "chunk_results": chunk_results
    }
```

#### 5.2.3 ä¸‰å…ƒç»„æ£€ç´¢ï¼ˆPath2ï¼‰

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬291è¡Œ)
def _triple_only_retrieval(self, question_embed: torch.Tensor) -> Dict:
    """ä¸‰å…ƒç»„å‘é‡æ£€ç´¢"""
    try:
        # 1. åœ¨ä¸‰å…ƒç»„ç´¢å¼•ä¸­æœç´¢
        scores, indices = self.triple_index.search(question_embed, top_k=self.top_k)

        # 2. è·å–ä¸‰å…ƒç»„å†…å®¹
        triples = []
        for idx in indices[0]:
            h, r, t = self.triple_map[str(idx)]
            triples.append((h, r, t))

        # 3. é‚»å±…æ‰©å±•
        neighbor_triples = []
        for h, r, t in triples:
            neighbor_triples.extend(self._collect_neighbor_triples(h))
            neighbor_triples.extend(self._collect_neighbor_triples(t))

        # 4. è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        scored_triples = self._calculate_triple_relevance_scores(
            question_embed, neighbor_triples, threshold=0.1
        )

        return {"scored_triples": scored_triples}
    except Exception as e:
        logger.error(f"Error in _triple_only_retrieval: {str(e)}")
        return {"scored_triples": []}
```

### 5.3 å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢æµç¨‹å›¾

```mermaid
flowchart TD
    A[é€šç”¨æŸ¥è¯¢] --> B[å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢]
    
    B --> C[Path1: èŠ‚ç‚¹å…³ç³»æ£€ç´¢]
    B --> D[Path2: ä¸‰å…ƒç»„æ£€ç´¢]
    
    C --> C1[FAISSèŠ‚ç‚¹æ£€ç´¢]
    C --> C2[å…³é”®è¯æ£€ç´¢]
    C --> C3[FAISSå…³ç³»æ£€ç´¢]
    C --> C4[Chunkè¯­ä¹‰æ£€ç´¢]
    
    D --> D1[ä¸‰å…ƒç»„å‘é‡æœç´¢]
    D --> D2[é‚»å±…æ‰©å±•]
    D --> D3[ç›¸å…³æ€§è®¡ç®—]
    
    C1 --> E[ç»“æœåˆå¹¶]
    C2 --> E
    C3 --> E
    C4 --> E
    D1 --> E
    D2 --> E
    D3 --> E
    
    E --> F[Chunk IDæå–]
    F --> G[æœ€ç»ˆç»“æœ]
    
    style A fill:#f3e5f5
    style B fill:#e8f5e8
    style E fill:#fff3e0
```

### 5.4 ä¼˜åŠ¿ä¸å±€é™

#### ä¼˜åŠ¿ï¼š
- âœ… **å…¨é¢è¦†ç›–**ï¼šå¤šç§æ£€ç´¢è·¯å¾„ç¡®ä¿ä¸é—æ¼ä¿¡æ¯
- âœ… **å¹¶è¡Œæ‰§è¡Œ**ï¼šæ˜¾è‘—æå‡æ£€ç´¢æ•ˆç‡
- âœ… **è¯­ä¹‰ç†è§£**ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦ï¼Œç†è§£æŸ¥è¯¢æ„å›¾
- âœ… **çµæ´»åŒ¹é…**ï¼šä¸ä¾èµ–å›ºå®šçš„æŸ¥è¯¢æ¨¡å¼

#### å±€é™ï¼š
- âŒ **è®¡ç®—å¯†é›†**ï¼šå‘é‡è®¡ç®—å’Œç›¸ä¼¼åº¦æ¯”è¾ƒè¾ƒæ…¢
- âŒ **é˜ˆå€¼æ•æ„Ÿ**ï¼šç›¸ä¼¼åº¦é˜ˆå€¼éš¾ä»¥è°ƒä¼˜
- âŒ **ç´¢å¼•æ›´æ–°**ï¼šéœ€è¦ç»´æŠ¤å¤šç§å‘é‡ç´¢å¼•

## 6. è·¯å¾„3ï¼šè¯­ä¹‰å‘é‡æ£€ç´¢

### 6.1 å·¥ä½œåŸç†

è¯­ä¹‰å‘é‡æ£€ç´¢åŸºäºæ–‡æ¡£å—çš„è¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡Œæ£€ç´¢ï¼Œé€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°ï¼š

1. **åŠ¨æ€ç´¢å¼•æ„å»º**ï¼šä»ç¼“å­˜çš„åµŒå…¥å‘é‡æ„å»ºå†…å­˜ç´¢å¼•
2. **è¯­ä¹‰æœç´¢**ï¼šåœ¨æ‰€æœ‰chunksä¸­æœç´¢æœ€ç›¸ä¼¼çš„
3. **è§„åˆ™åå¤‡**ï¼šå½“è¯­ä¹‰æœç´¢å¤±æ•ˆæ—¶ä½¿ç”¨è§„åˆ™åŒ¹é…

### 6.2 æ ¸å¿ƒä»£ç å®ç°

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬449è¡Œ)
def _chunk_embedding_retrieval(self, question_embed: torch.Tensor, top_k: int = 20) -> Dict:
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„chunkæ£€ç´¢"""
    if not self.chunk_embeddings_precomputed or self.chunk_faiss_index is None:
        return {"chunk_ids": [], "scores": [], "chunk_contents": []}

    # 1. FAISSæœç´¢
    query_embed_np = question_embed.cpu().numpy().reshape(1, -1).astype('float32')
    scores, indices = self.chunk_faiss_index.search(query_embed_np, top_k)

    # 2. æ”¶é›†ç»“æœ
    all_chunk_results = {}
    self._collect_chunk_results(all_chunk_results, scores[0], indices[0])

    # 3. è§„åˆ™åŒ¹é…åå¤‡
    original_query = getattr(self, '_current_query', '')
    if original_query:
        rule_based_results = self._rule_based_chunk_matching(original_query)
        for chunk_id, chunk_content in rule_based_results.items():
            if chunk_id in self.chunk_id_to_index:
                idx = self.chunk_id_to_index[chunk_id]
                all_chunk_results[idx] = {'chunk_id': chunk_id, 'score': 0.95}

    # 4. æ’åºå’Œè¿”å›
    sorted_results = sorted(all_chunk_results.items(), key=lambda x: x[1]['score'], reverse=True)[:top_k]

    chunk_ids = [result_data['chunk_id'] for _, result_data in sorted_results]
    similarity_scores = [result_data['score'] for _, result_data in sorted_results]
    chunk_contents = [self.chunk2id.get(chunk_id, f"[Missing content for chunk {chunk_id}]") for chunk_id in chunk_ids]

    return {
        "chunk_ids": chunk_ids,
        "scores": similarity_scores,
        "chunk_contents": chunk_contents
    }
```

## 7. ç»“æœèåˆä¸é™çº§ç­–ç•¥

### 7.1 èåˆç­–ç•¥æ¦‚è¿°

ç³»ç»Ÿé‡‡ç”¨æ™ºèƒ½èåˆæœºåˆ¶ï¼Œæ ¹æ®æ£€ç´¢ç­–ç•¥çš„æ‰§è¡Œæƒ…å†µåŠ¨æ€è°ƒæ•´å„è·¯å¾„çš„æƒé‡å’Œä¼˜å…ˆçº§ï¼š

1. **Graph-Firstä¼˜å…ˆ**ï¼šå½“Graph-Firstç­–ç•¥æˆåŠŸæ—¶ï¼Œä¼˜å…ˆä½¿ç”¨å…¶ç»“æœ
2. **é™æƒå¤„ç†**ï¼šå…¶ä»–è·¯å¾„çš„ç»“æœä¼šè¢«é™æƒå¤„ç†ï¼Œé¿å…å¹²æ‰°
3. **å›é€€æœºåˆ¶**ï¼šå½“ä¸»è¦ç­–ç•¥å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨å›é€€åˆ°å¤‡ç”¨ç­–ç•¥

### 7.2 æ ¸å¿ƒä»£ç å®ç°

#### 7.2.1 æ™ºèƒ½èåˆé€»è¾‘

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬2092è¡Œ)
def _collect_all_scored_triples(self, results: Dict, question_embed: torch.Tensor) -> List[Tuple[str, str, str, float]]:
    """Collect and merge all scored triples from both paths."""
    all_scored_triples = []
    
    # Check if Graph-First strategy was used (indicated by path1 having one_hop_triples)
    graph_first_used = results['path1_results'].get('one_hop_triples', [])
    
    if graph_first_used:
        # Graph-First strategy was used, prioritize its results
        logger.info(f"[GraphFirst] æ£€æµ‹åˆ°Graph-Firstç­–ç•¥ï¼Œä¼˜å…ˆä½¿ç”¨å…¶ä¸‰å…ƒç»„ç»“æœ")
        
        # Add path1 reranked triples (Graph-First results)
        path1_scored = self._rerank_triples_by_relevance(graph_first_used, question_embed)
        all_scored_triples.extend(path1_scored)
        
        # Add path2 scored triples but with reduced weight to avoid interference
        path2_scored = results['path2_results'].get('scored_triples', [])
        if path2_scored:
            # Reduce path2 scores to lower priority
            reduced_path2 = [(h, r, t, max(0.1, s * 0.3)) for (h, r, t, s) in path2_scored]
            all_scored_triples.extend(reduced_path2)
            logger.info(f"[GraphFirst] æ·»åŠ äº†{len(reduced_path2)}ä¸ªé™æƒçš„Path2ä¸‰å…ƒç»„")
    else:
        # Traditional retrieval, use both paths equally
        # Add path2 scored triples if available
        path2_scored = results['path2_results'].get('scored_triples', [])
        if path2_scored:
            all_scored_triples.extend(path2_scored)
        
        # Add path1 reranked triples
        path1_triples = results['path1_results'].get('one_hop_triples', [])
        if path1_triples:
            path1_scored = self._rerank_triples_by_relevance(path1_triples, question_embed)
            all_scored_triples.extend(path1_scored)

    # Add graph-first path_triples with higher base score to prioritize
    path_triples = results.get('path_triples', [])
    if path_triples:
        # Boost scores slightly to float to top
        boosted = [(h, r, t, min(0.99, (s + 0.2))) for (h, r, t, s) in path_triples]
        all_scored_triples.extend(boosted)
    
    # Sort by score (descending) and return top k
    all_scored_triples.sort(key=lambda x: x[3], reverse=True)
    return all_scored_triples
```

#### 7.2.2 é™çº§å›é€€ç­–ç•¥

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬645è¡Œ)
def retrieve(self, question: str) -> Dict:
    """ä¸»æ£€ç´¢å…¥å£ï¼Œæ™ºèƒ½é€‰æ‹©æ£€ç´¢ç­–ç•¥"""
    # æ£€æŸ¥æ˜¯å¦è§¦å‘Graph-Firstç­–ç•¥
    import re
    if re.search(r'[AB]æ ‹.*\d+å±‚|[AB]æ ‹.*åœ°ä¸‹ä¸€å±‚|[AB]æ ‹.*B1|Aæ ‹.*B1|Bæ ‹.*B1', question):
        logger.info(f"[GraphFirst] æ£€æµ‹åˆ°ä½ç½®æŸ¥è¯¢ï¼Œè§¦å‘Graph-Firstç­–ç•¥: {question}")
        try:
            # ä½¿ç”¨Graph-Firstç­–ç•¥
            graph_first_triples = self._path_strategy(question, question_embed)
            if graph_first_triples:
                logger.info(f"[GraphFirst] æˆåŠŸæ‰¾åˆ° {len(graph_first_triples)} ä¸ªä¸‰å…ƒç»„")
                # è¿”å›Graph-Firstç»“æœ
                return self._build_graph_first_result(graph_first_triples, question_embed)
            else:
                logger.info("[GraphFirst] æœªæ‰¾åˆ°ç›¸å…³ä¸‰å…ƒç»„ï¼Œå›é€€åˆ°åŸå§‹æ£€ç´¢")
        except Exception as e:
            logger.warning(f"[GraphFirst] æ‰§è¡Œå¤±è´¥: {e}ï¼Œå›é€€åˆ°åŸå§‹æ£€ç´¢")
    
    # å›é€€åˆ°å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢
    return self._parallel_dual_path_retrieval(question_embed, question)
```

#### 7.2.3 æƒé‡åˆ†é…æœºåˆ¶

```python
# æƒé‡åˆ†é…ç­–ç•¥
WEIGHT_STRATEGIES = {
    'graph_first_active': {
        'path1_weight': 1.0,      # Graph-Firstç»“æœï¼šæœ€é«˜æƒé‡
        'path2_weight': 0.3,      # å…¶ä»–è·¯å¾„ï¼šé™æƒå¤„ç†
        'path3_weight': 0.3       # è¯­ä¹‰æ£€ç´¢ï¼šé™æƒå¤„ç†
    },
    'traditional_retrieval': {
        'path1_weight': 0.85,     # èŠ‚ç‚¹å…³ç³»æ£€ç´¢ï¼šé«˜æƒé‡
        'path2_weight': 0.75,     # ä¸‰å…ƒç»„æ£€ç´¢ï¼šä¸­ç­‰æƒé‡
        'path3_weight': 0.65      # è¯­ä¹‰æ£€ç´¢ï¼šåŸºç¡€æƒé‡
    }
}

def _apply_weight_strategy(self, results: Dict, strategy: str) -> Dict:
    """åº”ç”¨æƒé‡ç­–ç•¥"""
    weights = WEIGHT_STRATEGIES.get(strategy, WEIGHT_STRATEGIES['traditional_retrieval'])
    
    # åº”ç”¨æƒé‡åˆ°å„ä¸ªè·¯å¾„çš„ç»“æœ
    if 'path1_results' in results:
        results['path1_results']['weight'] = weights['path1_weight']
    if 'path2_results' in results:
        results['path2_results']['weight'] = weights['path2_weight']
    if 'path3_results' in results:
        results['path3_results']['weight'] = weights['path3_weight']
    
    return results
```

### 7.3 èåˆç­–ç•¥æµç¨‹å›¾

```mermaid
flowchart TD
    A[æ£€ç´¢ç»“æœ] --> B{Graph-Firstæ¿€æ´»?}
    
    B -->|æ˜¯| C[Graph-Firstä¼˜å…ˆç­–ç•¥]
    B -->|å¦| D[ä¼ ç»Ÿæ£€ç´¢ç­–ç•¥]
    
    C --> C1[Path1: æƒé‡1.0]
    C --> C2[Path2: æƒé‡0.3]
    C --> C3[Path3: æƒé‡0.3]
    
    D --> D1[Path1: æƒé‡0.85]
    D --> D2[Path2: æƒé‡0.75]
    D --> D3[Path3: æƒé‡0.65]
    
    C1 --> E[ç»“æœåˆå¹¶]
    C2 --> E
    C3 --> E
    D1 --> E
    D2 --> E
    D3 --> E
    
    E --> F[å»é‡å¤„ç†]
    F --> G[åˆ†æ•°æ’åº]
    G --> H[æœ€ç»ˆç»“æœ]
    
    style C fill:#e1f5fe
    style D fill:#f3e5f5
    style E fill:#e8f5e8
```

## 8. å¤šå­é—®é¢˜å¤„ç†æœºåˆ¶

### 8.1 å·¥ä½œåŸç†

ç³»ç»Ÿæ”¯æŒå¤šå­é—®é¢˜çš„å¹¶è¡Œå¤„ç†ï¼Œé€šè¿‡ä»¥ä¸‹æœºåˆ¶å®ç°ï¼š

1. **å¹¶è¡Œå¤„ç†**ï¼šå¤šä¸ªå­é—®é¢˜åŒæ—¶æ‰§è¡Œæ£€ç´¢
2. **ç»“æœèšåˆ**ï¼šè‡ªåŠ¨åˆå¹¶æ‰€æœ‰å­é—®é¢˜çš„ç»“æœ
3. **å»é‡å¤„ç†**ï¼šç¡®ä¿æœ€ç»ˆç»“æœçš„å”¯ä¸€æ€§
4. **ç»Ÿè®¡æŠ¥å‘Š**ï¼šæä¾›æ¯ä¸ªå­é—®é¢˜çš„å¤„ç†ç»Ÿè®¡

### 8.2 æ ¸å¿ƒä»£ç å®ç°

#### 8.2.1 å¤šå­é—®é¢˜å¹¶è¡Œå¤„ç†

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬2243è¡Œ)
def process_subquestions_parallel(self, sub_questions: List[Dict], top_k: int = 10, involved_types: dict = None) -> Tuple[Dict, float]:
    """å¤šå­é—®é¢˜å¹¶è¡Œå¤„ç†"""
    start_time = time.time()
    
    default_max_workers = 4
    if self.config:
        default_max_workers = self.config.retrieval.faiss.max_workers
    max_workers = min(len(sub_questions), default_max_workers)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ä¸ºæ¯ä¸ªå­é—®é¢˜åˆ›å»ºä»»åŠ¡
        future_to_subquestion = {
            executor.submit(self._process_single_subquestion, sub_q, top_k, involved_types): sub_q 
            for sub_q in sub_questions
        }
        
        # åˆå§‹åŒ–èšåˆå®¹å™¨
        all_triples = set()
        all_chunk_ids = set()
        all_chunk_contents = {}
        all_sub_question_results = []
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in concurrent.futures.as_completed(future_to_subquestion):
            sub_q = future_to_subquestion[future]
            try:
                sub_result = future.result()
                
                with threading.Lock():
                    all_triples.update(sub_result['triples'])
                    all_chunk_ids.update(sub_result['chunk_ids'])
                    
                    for chunk_id, content in sub_result['chunk_contents'].items():
                        all_chunk_contents[chunk_id] = content
                    
                    all_sub_question_results.append(sub_result['sub_result'])
            except Exception as e:
                logger.error(f"Error processing sub-question: {str(e)}")
                with threading.Lock():
                    all_sub_question_results.append({
                        'sub_question': sub_q.get('sub-question', ''),
                        'triples_count': 0,
                        'chunk_ids_count': 0,
                        'time_taken': 0.0
                    })

    # å»é‡å¤„ç†
    dedup_triples = list(all_triples) 
    dedup_chunk_ids = list(all_chunk_ids)  
    
    dedup_chunk_contents = {chunk_id: all_chunk_contents.get(chunk_id, f"[Missing content for chunk {chunk_id}]") 
                           for chunk_id in dedup_chunk_ids}
    
    # å¤„ç†ç©ºç»“æœ
    if not dedup_triples and not dedup_chunk_contents:
        dedup_triples = ["No relevant information found"]
        dedup_chunk_contents = {"no_chunks": "No relevant chunks found"}
    
    total_time = time.time() - start_time
    
    return {
        'triples': dedup_triples,
        'chunk_ids': dedup_chunk_ids,
        'chunk_contents': dedup_chunk_contents,
        'sub_question_results': all_sub_question_results
    }, total_time
```

#### 8.2.2 å•å­é—®é¢˜å¤„ç†

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬2311è¡Œ)
def _process_single_subquestion(self, sub_question: Dict, top_k: int, involved_types: dict = None) -> Dict:
    """å¤„ç†å•ä¸ªå­é—®é¢˜"""
    sub_question_text = sub_question.get('sub-question', '')
    try:
        # æ‰§è¡Œæ£€ç´¢
        retrieval_results, time_taken = self.process_retrieval_results(sub_question_text, top_k, involved_types)
        
        # æå–ç»“æœ
        triples = retrieval_results.get('triples', []) or []
        chunk_ids = retrieval_results.get('chunk_ids', []) or []
        chunk_contents = retrieval_results.get('chunk_contents', []) or []
        
        # å¤„ç†chunk_contentsæ ¼å¼
        if isinstance(chunk_contents, dict):
            chunk_contents_list = list(chunk_contents.values())
        else:
            chunk_contents_list = chunk_contents
        
        # æ„å»ºå­é—®é¢˜ç»“æœ
        sub_result = {
            'sub_question': sub_question_text,
            'triples_count': len(triples),
            'chunk_ids_count': len(chunk_ids),
            'time_taken': time_taken
        }
        
        return {
            'triples': set(triples),
            'chunk_ids': set(chunk_ids),
            'chunk_contents': chunk_contents_dict,
            'sub_result': sub_result
        }
        
    except Exception as e:
        logger.error(f"Error processing sub-question '{sub_question_text}': {str(e)}")
        return {
            'triples': set(),
            'chunk_ids': set(),
            'chunk_contents': {},
            'sub_result': {
                'sub_question': sub_question_text,
                'triples_count': 0,
                'chunk_ids_count': 0,
                'time_taken': 0.0
            }
        }
```

### 8.3 å¤šå­é—®é¢˜å¤„ç†æµç¨‹å›¾

```mermaid
flowchart TD
    A[å¤šå­é—®é¢˜åˆ—è¡¨] --> B[å¹¶è¡Œä»»åŠ¡åˆ†é…]
    B --> C[å­é—®é¢˜1]
    B --> D[å­é—®é¢˜2]
    B --> E[å­é—®é¢˜N]
    
    C --> C1[æ£€ç´¢æ‰§è¡Œ]
    D --> D1[æ£€ç´¢æ‰§è¡Œ]
    E --> E1[æ£€ç´¢æ‰§è¡Œ]
    
    C1 --> F[ç»“æœæ”¶é›†]
    D1 --> F
    E1 --> F
    
    F --> G[ç»“æœèšåˆ]
    G --> H[å»é‡å¤„ç†]
    H --> I[ç»Ÿè®¡æŠ¥å‘Š]
    I --> J[æœ€ç»ˆç»“æœ]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style G fill:#e8f5e8
    style J fill:#fff3e0
```

### 8.4 ä¼˜åŠ¿ä¸ç‰¹ç‚¹

#### ä¼˜åŠ¿ï¼š
- âœ… **å¹¶è¡Œå¤„ç†**ï¼šæ˜¾è‘—æå‡å¤šå­é—®é¢˜çš„å¤„ç†æ•ˆç‡
- âœ… **è‡ªåŠ¨èšåˆ**ï¼šæ™ºèƒ½åˆå¹¶æ‰€æœ‰å­é—®é¢˜çš„ç»“æœ
- âœ… **å»é‡æœºåˆ¶**ï¼šç¡®ä¿æœ€ç»ˆç»“æœçš„å”¯ä¸€æ€§
- âœ… **é”™è¯¯å¤„ç†**ï¼šå•ä¸ªå­é—®é¢˜å¤±è´¥ä¸å½±å“æ•´ä½“å¤„ç†
- âœ… **ç»Ÿè®¡æŠ¥å‘Š**ï¼šæä¾›è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡ä¿¡æ¯

#### ç‰¹ç‚¹ï¼š
- ğŸ”„ **çº¿ç¨‹å®‰å…¨**ï¼šä½¿ç”¨é”æœºåˆ¶ç¡®ä¿å¹¶å‘å®‰å…¨
- ğŸ“Š **å®æ—¶ç»Ÿè®¡**ï¼šæ¯ä¸ªå­é—®é¢˜çš„å¤„ç†æ—¶é—´å’Œç»“æœæ•°é‡
- ğŸ›¡ï¸ **å®¹é”™æœºåˆ¶**ï¼šå•ä¸ªå­é—®é¢˜å¼‚å¸¸ä¸å½±å“å…¶ä»–å­é—®é¢˜
- ğŸ¯ **çµæ´»é…ç½®**ï¼šæ”¯æŒè‡ªå®šä¹‰å·¥ä½œçº¿ç¨‹æ•°é‡

## 9. åŠ¨æ€ç´¢å¼•ä¸ç¼“å­˜æœºåˆ¶

### 9.1 åŠ¨æ€ç´¢å¼•ç»“æ„

ç³»ç»Ÿé‡‡ç”¨åŠ¨æ€ç´¢å¼•æœºåˆ¶ï¼Œåœ¨è¿è¡Œæ—¶æ„å»ºå†…å­˜ç´¢å¼•ï¼Œæ”¯æŒçµæ´»çš„æ•°æ®æ›´æ–°ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  chunk_embedding_cache.pt                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Chunk ID 1  â”‚ Chunk ID 2  â”‚ Chunk ID 3  â”‚ Chunk ID 4  â”‚  â”‚
â”‚  â”‚   Vector    â”‚   Vector    â”‚   Vector    â”‚   Vector    â”‚  â”‚
â”‚  â”‚ [0.1,0.2..] â”‚ [0.3,0.4..] â”‚ [0.5,0.6..] â”‚ [0.7,0.8..] â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼ (ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 chunk_embedding_cache (å†…å­˜å­—å…¸)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Chunk ID 1  â”‚ Chunk ID 2  â”‚ Chunk ID 3  â”‚ Chunk ID 4  â”‚  â”‚
â”‚  â”‚  Tensor     â”‚  Tensor     â”‚  Tensor     â”‚  Tensor     â”‚  â”‚
â”‚  â”‚ [0.1,0.2..] â”‚ [0.3,0.4..] â”‚ [0.5,0.6..] â”‚ [0.7,0.8..] â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼ (åŠ¨æ€æ„å»ºç´¢å¼•)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    chunk_faiss_index                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Chunk 0   â”‚   Chunk 1   â”‚   Chunk 2   â”‚   Chunk 3   â”‚  â”‚
â”‚  â”‚  [0.1,0.2]  â”‚  [0.3,0.4]  â”‚  [0.5,0.6]  â”‚  [0.7,0.8]  â”‚  â”‚
â”‚  â”‚  [0.9,1.0]  â”‚  [1.1,1.2]  â”‚  [1.3,1.4]  â”‚  [1.5,1.6]  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 ç¼“å­˜æœºåˆ¶

#### 9.2.1 å¤šçº§ç¼“å­˜ç»“æ„

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬85è¡Œ)
class KTRetriever:
    def __init__(self, ...):
        # å¤šçº§ç¼“å­˜ç³»ç»Ÿ
        self.node_embedding_cache = {}        # èŠ‚ç‚¹åµŒå…¥ç¼“å­˜
        self.triple_embedding_cache = {}      # ä¸‰å…ƒç»„åµŒå…¥ç¼“å­˜
        self.query_embedding_cache = {}       # æŸ¥è¯¢åµŒå…¥ç¼“å­˜
        self.faiss_search_cache = {}          # FAISSæœç´¢ç¼“å­˜
        self.chunk_embedding_cache = {}       # ChunkåµŒå…¥ç¼“å­˜
        self.chunk_faiss_index = None         # Chunk FAISSç´¢å¼•
        self.chunk_id_to_index = {}           # Chunk IDåˆ°ç´¢å¼•æ˜ å°„
        self.index_to_chunk_id = {}           # ç´¢å¼•åˆ°Chunk IDæ˜ å°„
        
        # çº¿ç¨‹å®‰å…¨é”
        self.cache_locks = {
            'node_embedding': threading.RLock(),
            'triple_embedding': threading.RLock(),
            'query_embedding': threading.RLock(),
            'chunk_embedding': threading.RLock()  
        }
```

#### 9.2.2 ç¼“å­˜åŠ è½½ä¸æ„å»º

```python
# æ–‡ä»¶ï¼šmodels/retriever/enhanced_kt_retriever.py (ç¬¬3084è¡Œ)
def _precompute_chunk_embeddings(self):
    """é¢„è®¡ç®—chunkåµŒå…¥å‘é‡å¹¶æ„å»ºåŠ¨æ€ç´¢å¼•"""
    # 1. åŠ è½½ç¼“å­˜çš„åµŒå…¥å‘é‡
    if self._load_chunk_embedding_cache():
        logger.info("Successfully loaded chunk embeddings from disk cache")
        return

    # 2. è®¡ç®—åµŒå…¥å‘é‡
    chunk_ids = list(self.chunk2id.keys())
    chunk_texts = list(self.chunk2id.values())

    for i in range(0, len(chunk_texts), batch_size):
        batch_texts = chunk_texts[i:i + batch_size]
        batch_chunk_ids = chunk_ids[i:i + batch_size]

        batch_embeddings = self.qa_encoder.encode(batch_texts, convert_to_tensor=True)

        for j, chunk_id in enumerate(batch_chunk_ids):
            self.chunk_embedding_cache[chunk_id] = batch_embeddings[j]

    # 3. åŠ¨æ€æ„å»ºFAISSç´¢å¼•
    embeddings_list = []
    valid_chunk_ids = []

    for chunk_id, embed in self.chunk_embedding_cache.items():
        embeddings_list.append(embed.cpu().numpy())
        valid_chunk_ids.append(chunk_id)

    embeddings_array = np.array(embeddings_list)
    dimension = embeddings_array.shape[1]

    self.chunk_faiss_index = faiss.IndexFlatIP(dimension)
    self.chunk_faiss_index.add(embeddings_array.astype('float32'))
```

### 9.3 åŠ¨æ€ç´¢å¼•ä¼˜åŠ¿

- **çµæ´»æ€§**ï¼šå¯ä»¥æ ¹æ®æœ€æ–°ç¼“å­˜æ•°æ®é‡å»ºç´¢å¼•
- **å†…å­˜æ€§èƒ½**ï¼šå†…å­˜æœç´¢é€Ÿåº¦æ›´å¿«
- **ä¸€è‡´æ€§**ï¼šç´¢å¼•ä¸æ•°æ®å§‹ç»ˆåŒæ­¥
- **ç®€å•æ€§**ï¼šæ— éœ€å¤æ‚çš„ç´¢å¼•æ–‡ä»¶ç®¡ç†

## 10. å®é™…è¿è¡Œç¤ºä¾‹

### 10.1 Graph-Firstç­–ç•¥ç¤ºä¾‹

#### æŸ¥è¯¢ï¼š"Aæ ‹3å±‚æœ‰å“ªäº›è®¾å¤‡"

**æ‰§è¡Œæµç¨‹**ï¼š
1. **ç­–ç•¥é€‰æ‹©**ï¼šæ£€æµ‹åˆ°ä½ç½®æŸ¥è¯¢ï¼Œè§¦å‘Graph-Firstç­–ç•¥
2. **ä½ç½®åˆ†æ**ï¼šæå–å»ºç­‘="A"ï¼Œæ¥¼å±‚="3å±‚"
3. **ç­–ç•¥ç”Ÿæˆ**ï¼šç”Ÿæˆç›®æ ‡èŠ‚ç‚¹åˆ—è¡¨å’Œæœç´¢ç­–ç•¥
4. **å›¾éå†**ï¼šä»é”šç‚¹èŠ‚ç‚¹å¼€å§‹éå†ï¼Œæ‰¾åˆ°ç›¸å…³è®¾å¤‡
5. **ç»“æœè¿”å›**ï¼šè¿”å›ç²¾ç¡®çš„ä¸‰å…ƒç»„å’Œchunk_ids

**æ—¥å¿—è¾“å‡º**ï¼š
```
[GraphFirst] æ£€æµ‹åˆ°ä½ç½®æŸ¥è¯¢ï¼Œè§¦å‘Graph-Firstç­–ç•¥: Aæ ‹3å±‚æœ‰å“ªäº›è®¾å¤‡
[GraphFirst] æ™ºèƒ½åˆ†æä½ç½®ä¿¡æ¯: {'buildings': ['A'], 'floors': ['3å±‚'], 'rooms': [], 'locations': [], 'keywords': []}
[GraphFirst] å›¾è°±ä½ç½®æ¨¡å¼: {'building_nodes': [...], 'floor_nodes': [...], 'location_nodes': [...]}
[GraphFirst] åŠ¨æ€æ£€ç´¢ç­–ç•¥: {'search_type': 'building_floor_equipment', 'target_nodes': [...], 'equipment_filters': [...]}
[GraphFirst] æˆåŠŸæ‰¾åˆ° 3 ä¸ªä¸‰å…ƒç»„
```

**è¿”å›ç»“æœ**ï¼š
- ä¸‰å…ƒç»„ï¼š`(Aæ ‹3å±‚ç©ºè°ƒç®±, located_in, LOC-A-03-AHU)`
- Chunk IDsï¼š`['yHxZDE9p', '7Y5zNYNc', 'oW5zndt1']`
- è®¾å¤‡ï¼šAæ ‹3å±‚ç©ºè°ƒç®±ã€Aæ ‹3å±‚ç…§æ˜é…ç”µç®±ã€Aæ ‹3å±‚01å·å˜é£é‡æœ«ç«¯

### 10.2 å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢ç¤ºä¾‹

#### æŸ¥è¯¢ï¼š"ç©ºè°ƒç³»ç»Ÿçš„å·¥ä½œåŸç†"

**æ‰§è¡Œæµç¨‹**ï¼š
1. **ç­–ç•¥é€‰æ‹©**ï¼šä¸æ»¡è¶³Graph-Firstæ¡ä»¶ï¼Œä½¿ç”¨å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢
2. **å¹¶è¡Œæ‰§è¡Œ**ï¼šåŒæ—¶æ‰§è¡ŒFAISSã€å…³é”®è¯ã€ä¸‰å…ƒç»„ã€Chunkæ£€ç´¢
3. **ç»“æœèåˆ**ï¼šåˆå¹¶æ‰€æœ‰è·¯å¾„çš„ç»“æœ
4. **æƒé‡åˆ†é…**ï¼šæ ¹æ®è·¯å¾„ç‰¹æ€§åˆ†é…ä¸åŒæƒé‡

**è¿”å›ç»“æœ**ï¼š
- Path1ç»“æœï¼šåŸºäºFAISSå’Œå…³é”®è¯çš„èŠ‚ç‚¹æ£€ç´¢
- Path2ç»“æœï¼šåŸºäºä¸‰å…ƒç»„å‘é‡ç›¸ä¼¼åº¦çš„æ£€ç´¢
- Path3ç»“æœï¼šåŸºäºChunkè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ£€ç´¢
- æœ€ç»ˆç»“æœï¼šå»é‡åçš„ç»¼åˆç»“æœ

### 10.3 å¤šå­é—®é¢˜å¤„ç†ç¤ºä¾‹

#### è¾“å…¥ï¼šå¤šä¸ªå­é—®é¢˜
```python
sub_questions = [
    {"sub-question": "Aæ ‹3å±‚æœ‰å“ªäº›è®¾å¤‡ï¼Ÿ"},
    {"sub-question": "Bæ ‹åœ°ä¸‹ä¸€å±‚æœ‰å“ªäº›è®¾å¤‡ï¼Ÿ"},
    {"sub-question": "ç©ºè°ƒç³»ç»ŸåŒ…å«å“ªäº›ç»„ä»¶ï¼Ÿ"}
]
```

**æ‰§è¡Œæµç¨‹**ï¼š
1. **å¹¶è¡Œåˆ†é…**ï¼šä¸ºæ¯ä¸ªå­é—®é¢˜åˆ›å»ºç‹¬ç«‹ä»»åŠ¡
2. **å¹¶è¡Œæ‰§è¡Œ**ï¼šåŒæ—¶å¤„ç†æ‰€æœ‰å­é—®é¢˜
3. **ç»“æœèšåˆ**ï¼šåˆå¹¶æ‰€æœ‰å­é—®é¢˜çš„ç»“æœ
4. **å»é‡å¤„ç†**ï¼šç¡®ä¿æœ€ç»ˆç»“æœçš„å”¯ä¸€æ€§

**è¿”å›ç»“æœ**ï¼š
- æ€»ä¸‰å…ƒç»„æ•°é‡ï¼šå»é‡åçš„å”¯ä¸€ä¸‰å…ƒç»„
- æ€»Chunkæ•°é‡ï¼šå»é‡åçš„å”¯ä¸€Chunk IDs
- å­é—®é¢˜ç»Ÿè®¡ï¼šæ¯ä¸ªå­é—®é¢˜çš„å¤„ç†æ—¶é—´å’Œç»“æœæ•°é‡

## 11. æ ¸å¿ƒä¼˜åŠ¿åˆ†æ

### 11.1 æ™ºèƒ½ç­–ç•¥é€‰æ‹©ä¼˜åŠ¿

1. **è‡ªé€‚åº”é€‰æ‹©**ï¼š
   - æ ¹æ®æŸ¥è¯¢ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ£€ç´¢ç­–ç•¥
   - Graph-Firstç­–ç•¥ä¸“ä¸ºä½ç½®æŸ¥è¯¢ä¼˜åŒ–
   - å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢è¦†ç›–é€šç”¨æŸ¥è¯¢åœºæ™¯

2. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - ä½ç½®æŸ¥è¯¢ï¼šGraph-Firstç­–ç•¥å¿«é€Ÿç²¾ç¡®
   - é€šç”¨æŸ¥è¯¢ï¼šå¤šè·¯å¾„å¹¶è¡Œç¡®ä¿å…¨é¢è¦†ç›–
   - æ™ºèƒ½å›é€€ï¼šç¡®ä¿ç³»ç»Ÿé²æ£’æ€§

### 11.2 å¤šè·¯å¾„äº’è¡¥ä¼˜åŠ¿

1. **ç»“æ„åŒ– vs è¯­ä¹‰åŒ–**ï¼š
   - Graph-Firstï¼šç»“æ„åŒ–æŸ¥è¯¢çš„ç²¾ç¡®åŒ¹é…
   - å¤šè·¯å¾„æ£€ç´¢ï¼šæ¨¡ç³ŠæŸ¥è¯¢çš„è¯­ä¹‰ç†è§£

2. **è§„åˆ™ vs å­¦ä¹ **ï¼š
   - Graph-Firstï¼šåŸºäºè§„åˆ™çš„ç¡®å®šæ€§æ¨ç†
   - å¤šè·¯å¾„æ£€ç´¢ï¼šåŸºäºå­¦ä¹ çš„æ¦‚ç‡æ€§æ¨ç†

3. **å¿«é€Ÿ vs å…¨é¢**ï¼š
   - Graph-Firstï¼šå¿«é€Ÿä½†å¯èƒ½é—æ¼
   - å¤šè·¯å¾„æ£€ç´¢ï¼šå…¨é¢ä½†è®¡ç®—å¯†é›†

### 11.3 æ™ºèƒ½èåˆä¼˜åŠ¿

1. **åŠ¨æ€æƒé‡åˆ†é…**ï¼š
   - Graph-Firstæ¿€æ´»æ—¶ï¼šä¼˜å…ˆä½¿ç”¨å…¶ç²¾ç¡®ç»“æœ
   - å…¶ä»–è·¯å¾„é™æƒï¼šé¿å…å¹²æ‰°ä¸»è¦ç»“æœ
   - ä¼ ç»Ÿæ£€ç´¢æ—¶ï¼šå‡è¡¡ä½¿ç”¨å„è·¯å¾„ç»“æœ

2. **é™çº§å›é€€æœºåˆ¶**ï¼š
   - ä¸»è¦ç­–ç•¥å¤±è´¥æ—¶è‡ªåŠ¨å›é€€
   - ç¡®ä¿ç³»ç»Ÿå§‹ç»ˆèƒ½è¿”å›ç»“æœ
   - æä¾›å¤šå±‚æ¬¡çš„å®¹é”™ä¿éšœ

### 11.4 å¹¶è¡Œå¤„ç†ä¼˜åŠ¿

1. **å¤šå­é—®é¢˜å¹¶è¡Œ**ï¼š
   - æ˜¾è‘—æå‡å¤šå­é—®é¢˜çš„å¤„ç†æ•ˆç‡
   - çº¿ç¨‹å®‰å…¨çš„ç»“æœèšåˆ
   - æ™ºèƒ½å»é‡ç¡®ä¿ç»“æœå”¯ä¸€æ€§

2. **å¤šè·¯å¾„å¹¶è¡Œ**ï¼š
   - åŒæ—¶æ‰§è¡Œå¤šç§æ£€ç´¢ç­–ç•¥
   - æœ€å¤§åŒ–åˆ©ç”¨ç³»ç»Ÿèµ„æº
   - æ˜¾è‘—é™ä½æ€»ä½“å“åº”æ—¶é—´

### 11.5 åŠ¨æ€ç´¢å¼•ä¼˜åŠ¿

1. **å†…å­˜æ€§èƒ½**ï¼š
   - å†…å­˜æœç´¢æ¯”ç£ç›˜æœç´¢å¿«æ•°å€
   - é¿å…å¤§å‹ç´¢å¼•æ–‡ä»¶çš„ç£ç›˜å ç”¨
   - æ”¯æŒå®æ—¶æ•°æ®æ›´æ–°

2. **çµæ´»ç®¡ç†**ï¼š
   - å¯ä»¥æ ¹æ®æœ€æ–°ç¼“å­˜æ•°æ®é‡å»ºç´¢å¼•
   - æ— éœ€å¤æ‚çš„ç´¢å¼•æ–‡ä»¶ç®¡ç†
   - ç´¢å¼•ä¸æ•°æ®å§‹ç»ˆåŒæ­¥

### 11.6 ç³»ç»Ÿæ¶æ„ä¼˜åŠ¿

1. **æ¨¡å—åŒ–è®¾è®¡**ï¼š
   - å„æ£€ç´¢ç­–ç•¥ç‹¬ç«‹å¯æµ‹è¯•
   - æ˜“äºæ‰©å±•æ–°çš„æ£€ç´¢æ–¹æ³•
   - æ¸…æ™°çš„èŒè´£åˆ†ç¦»

2. **å¯é…ç½®æ€§**ï¼š
   - æ”¯æŒè‡ªå®šä¹‰å·¥ä½œçº¿ç¨‹æ•°é‡
   - å¯è°ƒæ•´çš„æƒé‡å’Œé˜ˆå€¼å‚æ•°
   - çµæ´»çš„ç¼“å­˜ç­–ç•¥é…ç½®

### 11.7 å®é™…åº”ç”¨ä¼˜åŠ¿

1. **é¢†åŸŸç‰¹åŒ–**ï¼š
   - ä¸“ä¸ºå»ºç­‘èµ„äº§ç®¡ç†é¢†åŸŸè®¾è®¡
   - æ·±åº¦ç†è§£ä½ç½®å’Œè®¾å¤‡å…³ç³»
   - ä¼˜åŒ–çš„æŸ¥è¯¢æ¨¡å¼è¯†åˆ«

2. **ç”¨æˆ·ä½“éªŒ**ï¼š
   - å¿«é€Ÿå“åº”çš„ä½ç½®æŸ¥è¯¢
   - å…¨é¢çš„ä¿¡æ¯è¦†ç›–
   - æ™ºèƒ½çš„ç»“æœæ’åºå’Œå»é‡

## 12. æ€»ç»“

YoutuGraphRAGå¤šè·¯æ··åˆæ£€ç´¢ç³»ç»Ÿé€šè¿‡æ™ºèƒ½ç­–ç•¥é€‰æ‹©ã€å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢ã€æ™ºèƒ½èåˆæœºåˆ¶å’ŒåŠ¨æ€ç´¢å¼•ç­‰æŠ€æœ¯ï¼Œå®ç°äº†é«˜æ•ˆã€å‡†ç¡®ã€å…¨é¢çš„ä¿¡æ¯æ£€ç´¢ã€‚ç³»ç»Ÿç‰¹åˆ«é’ˆå¯¹å»ºç­‘èµ„äº§ç®¡ç†é¢†åŸŸè¿›è¡Œäº†ä¼˜åŒ–ï¼Œèƒ½å¤Ÿæ™ºèƒ½è¯†åˆ«ä½ç½®æŸ¥è¯¢å¹¶ä¼˜å…ˆä½¿ç”¨Graph-Firstç­–ç•¥ï¼ŒåŒæ—¶é€šè¿‡å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢ç¡®ä¿é€šç”¨æŸ¥è¯¢çš„å…¨é¢è¦†ç›–ã€‚

### 12.1 æ ¸å¿ƒåˆ›æ–°ç‚¹

1. **Graph-Firstæ™ºèƒ½ç­–ç•¥**ï¼šä¸“ä¸ºä½ç½®æŸ¥è¯¢ä¼˜åŒ–çš„å›¾éå†æ£€ç´¢
2. **æ™ºèƒ½ç­–ç•¥é€‰æ‹©**ï¼šæ ¹æ®æŸ¥è¯¢ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ£€ç´¢æ–¹æ³•
3. **åŠ¨æ€æƒé‡èåˆ**ï¼šæ ¹æ®ç­–ç•¥æ‰§è¡Œæƒ…å†µåŠ¨æ€è°ƒæ•´ç»“æœæƒé‡
4. **å¤šå­é—®é¢˜å¹¶è¡Œ**ï¼šé«˜æ•ˆå¤„ç†å¤æ‚çš„å¤šå­é—®é¢˜åœºæ™¯
5. **åŠ¨æ€ç´¢å¼•æœºåˆ¶**ï¼šå†…å­˜ç´¢å¼•æä¾›æœ€ä½³æ€§èƒ½

### 12.2 æŠ€æœ¯ç‰¹è‰²

- **æ™ºèƒ½æ€§**ï¼šè‡ªé€‚åº”ç­–ç•¥é€‰æ‹©å’Œæƒé‡åˆ†é…
- **é«˜æ•ˆæ€§**ï¼šå¹¶è¡Œå¤„ç†å’ŒåŠ¨æ€ç´¢å¼•ä¼˜åŒ–
- **é²æ£’æ€§**ï¼šå¤šå±‚å›é€€å’Œå®¹é”™æœºåˆ¶
- **å¯æ‰©å±•æ€§**ï¼šæ¨¡å—åŒ–è®¾è®¡æ”¯æŒåŠŸèƒ½æ‰©å±•
- **é¢†åŸŸç‰¹åŒ–**ï¼šæ·±åº¦ç†è§£å»ºç­‘èµ„äº§ç®¡ç†åœºæ™¯

### 12.3 åº”ç”¨ä»·å€¼

è¯¥ç³»ç»Ÿä¸ºå»ºç­‘èµ„äº§ç®¡ç†æä¾›äº†å¼ºå¤§çš„ä¿¡æ¯æ£€ç´¢èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¿«é€Ÿå‡†ç¡®åœ°å›ç­”ä½ç½®ç›¸å…³çš„è®¾å¤‡æŸ¥è¯¢ï¼ŒåŒæ—¶é€šè¿‡å¤šè·¯å¾„æ£€ç´¢ç¡®ä¿ä¿¡æ¯çš„å…¨é¢è¦†ç›–ã€‚ç³»ç»Ÿçš„æ™ºèƒ½åŒ–å’Œå¹¶è¡ŒåŒ–è®¾è®¡ä½¿å…¶èƒ½å¤Ÿæ»¡è¶³å¤§è§„æ¨¡ã€é«˜å¹¶å‘çš„å®é™…åº”ç”¨éœ€æ±‚ã€‚

---

*æœ¬æ–‡æ¡£åŸºäºYoutuGraphRAGé¡¹ç›®çš„å®é™…ä»£ç åˆ†æå’Œè¿è¡Œæµ‹è¯•æ•´ç†è€Œæˆï¼Œè¯¦ç»†é˜è¿°äº†å¤šè·¯æ··åˆæ£€ç´¢ç³»ç»Ÿçš„è®¾è®¡ç†å¿µã€å®ç°æœºåˆ¶å’Œæ€§èƒ½ä¼˜åŠ¿ã€‚é€šè¿‡ç»“åˆç°æœ‰ä»£ç çš„å®é™…é€»è¾‘ï¼Œä¸ºåç»­çš„ç³»ç»Ÿè¿­ä»£å’Œä¼˜åŒ–æä¾›äº†æ¸…æ™°çš„æŠ€æœ¯æŒ‡å¯¼ã€‚*
        return

    # 2. è®¡ç®—åµŒå…¥å‘é‡
    chunk_ids = list(self.chunk2id.keys())
    chunk_texts = list(self.chunk2id.values())

    for i in range(0, len(chunk_texts), batch_size):
        batch_texts = chunk_texts[i:i + batch_size]
        batch_chunk_ids = chunk_ids[i:i + batch_size]

        batch_embeddings = self.qa_encoder.encode(batch_texts, convert_to_tensor=True)

        for j, chunk_id in enumerate(batch_chunk_ids):
            self.chunk_embedding_cache[chunk_id] = batch_embeddings[j]

    # 3. åŠ¨æ€æ„å»ºFAISSç´¢å¼•
    embeddings_list = []
    valid_chunk_ids = []

    for chunk_id, embed in self.chunk_embedding_cache.items():
        embeddings_list.append(embed.cpu().numpy())
        valid_chunk_ids.append(chunk_id)

    embeddings_array = np.array(embeddings_list)
    dimension = embeddings_array.shape[1]

    self.chunk_faiss_index = faiss.IndexFlatIP(dimension)
    self.chunk_faiss_index.add(embeddings_array.astype('float32'))
```

#### 5.2.2 è¯­ä¹‰æ£€ç´¢

```python
def _chunk_embedding_retrieval(self, question_embed: torch.Tensor, top_k: int = 20) -> Dict:
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„chunkæ£€ç´¢"""
    if not self.chunk_embeddings_precomputed or self.chunk_faiss_index is None:
        return {"chunk_ids": [], "scores": [], "chunk_contents": []}

    # 1. FAISSæœç´¢
    query_embed_np = question_embed.cpu().numpy().reshape(1, -1).astype('float32')
    scores, indices = self.chunk_faiss_index.search(query_embed_np, top_k)

    # 2. æ”¶é›†ç»“æœ
    all_chunk_results = {}
    self._collect_chunk_results(all_chunk_results, scores[0], indices[0])

    # 3. è§„åˆ™åŒ¹é…åå¤‡
    original_query = getattr(self, '_current_query', '')
    if original_query:
        rule_based_results = self._rule_based_chunk_matching(original_query)
        for chunk_id, chunk_content in rule_based_results.items():
            if chunk_id in self.chunk_id_to_index:
                idx = self.chunk_id_to_index[chunk_id]
                all_chunk_results[idx] = {'chunk_id': chunk_id, 'score': 0.95}

    # 4. æ’åºå’Œè¿”å›
    sorted_results = sorted(all_chunk_results.items(), key=lambda x: x[1]['score'], reverse=True)[:top_k]

    chunk_ids = [result_data['chunk_id'] for _, result_data in sorted_results]
    similarity_scores = [result_data['score'] for _, result_data in sorted_results]
    chunk_contents = [self.chunk2id.get(chunk_id, f"[Missing content for chunk {chunk_id}]") for chunk_id in chunk_ids]

    return {
        "chunk_ids": chunk_ids,
        "scores": similarity_scores,
        "chunk_contents": chunk_contents
    }
```

## 6. åŠ¨æ€ç´¢å¼•æœºåˆ¶

### 6.1 åŠ¨æ€ç´¢å¼•ç»“æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  chunk_embedding_cache.pt                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Chunk ID 1  â”‚ Chunk ID 2  â”‚ Chunk ID 3  â”‚ Chunk ID 4  â”‚  â”‚
â”‚  â”‚   Vector    â”‚   Vector    â”‚   Vector    â”‚   Vector    â”‚  â”‚
â”‚  â”‚ [0.1,0.2..] â”‚ [0.3,0.4..] â”‚ [0.5,0.6..] â”‚ [0.7,0.8..] â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼ (ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 chunk_embedding_cache (å†…å­˜å­—å…¸)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Chunk ID 1  â”‚ Chunk ID 2  â”‚ Chunk ID 3  â”‚ Chunk ID 4  â”‚  â”‚
â”‚  â”‚  Tensor     â”‚  Tensor     â”‚  Tensor     â”‚  Tensor     â”‚  â”‚
â”‚  â”‚ [0.1,0.2..] â”‚ [0.3,0.4..] â”‚ [0.5,0.6..] â”‚ [0.7,0.8..] â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼ (åŠ¨æ€æ„å»ºç´¢å¼•)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    chunk_faiss_index                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Chunk 0   â”‚   Chunk 1   â”‚   Chunk 2   â”‚   Chunk 3   â”‚  â”‚
â”‚  â”‚  [0.1,0.2]  â”‚  [0.3,0.4]  â”‚  [0.5,0.6]  â”‚  [0.7,0.8]  â”‚  â”‚
â”‚  â”‚  [0.9,1.0]  â”‚  [1.1,1.2]  â”‚  [1.3,1.4]  â”‚  [1.5,1.6]  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 åŠ¨æ€ç´¢å¼•ä¼˜åŠ¿

- **çµæ´»æ€§**ï¼šå¯ä»¥æ ¹æ®æœ€æ–°ç¼“å­˜æ•°æ®é‡å»ºç´¢å¼•
- **å†…å­˜æ€§èƒ½**ï¼šå†…å­˜æœç´¢é€Ÿåº¦æ›´å¿«
- **ä¸€è‡´æ€§**ï¼šç´¢å¼•ä¸æ•°æ®å§‹ç»ˆåŒæ­¥
- **ç®€å•æ€§**ï¼šæ— éœ€å¤æ‚çš„ç´¢å¼•æ–‡ä»¶ç®¡ç†

## 7. èŠ‚ç‚¹ä¸è¾¹ç»“æ„

### 7.1 èŠ‚ç‚¹æ¦‚å¿µä¸åˆ†ç±»

#### 7.1.1 èŠ‚ç‚¹ç±»å‹å±‚æ¬¡

YoutuGraphRAG ä¸­çš„èŠ‚ç‚¹æŒ‰ç…§åŠŸèƒ½å’Œå±‚æ¬¡åˆ†ä¸ºå››ä¸ªçº§åˆ«ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      èŠ‚ç‚¹ç±»å‹å±‚æ¬¡ç»“æ„                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Level 4: Community (ç¤¾åŒºèŠ‚ç‚¹)                               â”‚
â”‚  â€¢ ä¸»é¢˜ç¤¾åŒºï¼Œè¿æ¥ç›¸å…³å®ä½“å’Œå…³é”®è¯                             â”‚
â”‚  â€¢ ç¤ºä¾‹: "HVACç³»ç»Ÿç¤¾åŒº", "å»ºç­‘èµ„äº§ç®¡ç†ç¤¾åŒº"                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Level 3: Keyword (å…³é”®è¯èŠ‚ç‚¹)                               â”‚
â”‚  â€¢ æ–‡æ¡£ä¸­çš„é‡è¦æœ¯è¯­å’Œæ¦‚å¿µ                                   â”‚
â”‚  â€¢ ç¤ºä¾‹: "ç©ºè°ƒç³»ç»Ÿ", "é…ç”µè®¾å¤‡", "æ¥¼å±‚ç®¡ç†"                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Level 2: Entity (å®ä½“èŠ‚ç‚¹)                                 â”‚
â”‚  â€¢ å®é™…çš„ç‰©ç†/é€»è¾‘å¯¹è±¡                                     â”‚
â”‚  â€¢ ç¤ºä¾‹: "Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº", "Aæ ‹ä¸‰å±‚", "HVACç³»ç»Ÿ"            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Level 1: Attribute (å±æ€§èŠ‚ç‚¹)                              â”‚
â”‚  â€¢ å®ä½“çš„å±æ€§å’Œç‰¹å¾å€¼                                      â”‚
â”‚  â€¢ ç¤ºä¾‹: "asset_id: A-CH-01", "manufacturer: Johnson Controls" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 7.1.2 èŠ‚ç‚¹ç±»å‹å®šä¹‰

æ ¹æ® Schema æ–‡ä»¶å®šä¹‰çš„èŠ‚ç‚¹ç±»å‹ï¼š

```json
// schemas/building_assets.json
{
  "Nodes": [
    "asset",           // èµ„äº§è®¾å¤‡
    "system",          // ç³»ç»Ÿï¼ˆå¦‚HVACç³»ç»Ÿï¼‰
    "location",        // ä½ç½®ï¼ˆå¦‚æˆ¿é—´ã€æ¥¼å±‚ï¼‰
    "building",        // å»ºç­‘
    "floor",           // æ¥¼å±‚
    "room",            // æˆ¿é—´
    "manufacturer",    // åˆ¶é€ å•†
    "model",           // è®¾å¤‡å‹å·
    "equipment_type"   // è®¾å¤‡ç±»å‹
  ]
}
```

### 7.2 èŠ‚ç‚¹æ•°æ®ç»“æ„

#### 7.2.1 å®ä½“èŠ‚ç‚¹ç»“æ„

**å­˜å‚¨ç»“æ„**ï¼š
```json
{
  "label": "entity",
  "properties": {
    "name": "Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº",        // å®ä½“åç§°ï¼ˆæ ¸å¿ƒæ ‡è¯†ï¼‰
    "schema_type": "asset",          // å®ä½“ç±»å‹
    "chunk id": "6nphZ9wJ",          // æ¥æºæ–‡æ¡£å—ID
    "description": "ç¦»å¿ƒå¼å†·æœº",      // å®ä½“æè¿°ï¼ˆå¯é€‰ï¼‰
    "alias": ["Aæ ‹å†·æœº1å·", "1å·å†·æœº"], // åˆ«åï¼ˆå¯é€‰ï¼‰
    "location_id": "LOC-A-B1-MECH"    // ä½ç½®ç¼–ç ï¼ˆå¯é€‰ï¼‰
  },
  "level": 2                          // èŠ‚ç‚¹å±‚æ¬¡çº§åˆ«
}
```

**NetworkXä¸­çš„è¡¨ç¤º**ï¼š
```python
# èŠ‚ç‚¹ID: "entity_0"
node_data = {
    "label": "entity",
    "properties": {
        "name": "Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº",
        "schema_type": "asset",
        "chunk id": "6nphZ9wJ"
    },
    "level": 2
}
```

#### 7.2.2 å±æ€§èŠ‚ç‚¹ç»“æ„

**å­˜å‚¨ç»“æ„**ï¼š
```json
{
  "label": "attribute",
  "properties": {
    "name": "asset_id: A-CH-01",      // å±æ€§å:å±æ€§å€¼
    "chunk id": "6nphZ9wJ"            // æ¥æºæ–‡æ¡£å—ID
  },
  "level": 1                          // èŠ‚ç‚¹å±‚æ¬¡çº§åˆ«
}
```

**å±æ€§èŠ‚ç‚¹çš„ç‰¹ç‚¹**ï¼š
- **å±æ€§æ ¼å¼**ï¼š`"å±æ€§å: å±æ€§å€¼"`ï¼ˆå¦‚ `"asset_id: A-CH-01"`ï¼‰
- **è¿æ¥å…³ç³»**ï¼šé€šè¿‡ `has_attribute` å…³ç³»è¿æ¥åˆ°å®ä½“èŠ‚ç‚¹
- **æ•°æ®ç±»å‹**ï¼šå­˜å‚¨å„ç§ç±»å‹çš„å±æ€§å€¼ï¼ˆå­—ç¬¦ä¸²ã€æ•°å­—ã€æ—¥æœŸç­‰ï¼‰

#### 7.2.3 å…³é”®è¯èŠ‚ç‚¹ç»“æ„

**å­˜å‚¨ç»“æ„**ï¼š
```json
{
  "label": "keyword",
  "properties": {
    "name": "ç©ºè°ƒç³»ç»Ÿ",                // å…³é”®è¯åç§°
    "chunk id": "YZ1N7DbR"            // æ¥æºæ–‡æ¡£å—ID
  },
  "level": 3                          // èŠ‚ç‚¹å±‚æ¬¡çº§åˆ«
}
```

#### 7.2.4 ç¤¾åŒºèŠ‚ç‚¹ç»“æ„

**å­˜å‚¨ç»“æ„**ï¼š
```json
{
  "label": "community",
  "properties": {
    "name": "HVACç³»ç»Ÿç¤¾åŒº",            // ç¤¾åŒºåç§°
    "description": "æš–é€šç©ºè°ƒç›¸å…³è®¾å¤‡å’Œç³»ç»Ÿ" // ç¤¾åŒºæè¿°
  },
  "level": 4                          // èŠ‚ç‚¹å±‚æ¬¡çº§åˆ«
}
```

### 7.3 è¾¹çš„æ•°æ®ç»“æ„

#### 7.3.1 è¾¹çš„åŸºæœ¬ç»“æ„

å›¾è°±ä¸­çš„è¾¹é‡‡ç”¨ä¸‰å…ƒç»„ç»“æ„å­˜å‚¨ï¼ŒåŒ…å«èµ·ç‚¹ã€å…³ç³»ã€ç»ˆç‚¹ä¸‰ä¸ªæ ¸å¿ƒå…ƒç´ ï¼š

```
å®ä½“èŠ‚ç‚¹ â†’ å…³ç³»ç±»å‹ â†’ å®ä½“èŠ‚ç‚¹/å±æ€§èŠ‚ç‚¹
```

#### 7.3.2 å®ä½“å…³ç³»è¾¹

**JSONæ ¼å¼**ï¼š
```json
{
  "start_node": {
    "label": "entity",
    "properties": {
      "name": "Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº",
      "schema_type": "asset",
      "chunk id": "6nphZ9wJ"
    }
  },
  "relation": "located_in",           // å…³ç³»ç±»å‹
  "end_node": {
    "label": "entity",
    "properties": {
      "name": "LOC-A-B1-MECH",
      "schema_type": "location",
      "chunk id": "6nphZ9wJ"
    }
  }
}
```

**NetworkXä¸­çš„è¡¨ç¤º**ï¼š
```python
# è¾¹æ•°æ®ç»“æ„
edge_data = {
    "relation": "located_in",      // ä¸»è¦å…³ç³»ç±»å‹
    "label": "located_in"          // å¤‡ç”¨æ ‡ç­¾
}

# è¾¹è¿æ¥
graph.add_edge("entity_0", "entity_1", **edge_data)
```

#### 7.3.3 å±æ€§è¿æ¥è¾¹

**JSONæ ¼å¼**ï¼š
```json
{
  "start_node": {
    "label": "entity",
    "properties": {
      "name": "Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº",
      "schema_type": "asset"
    }
  },
  "relation": "has_attribute",       // å±æ€§è¿æ¥å…³ç³»
  "end_node": {
    "label": "attribute",
    "properties": {
      "name": "asset_id: A-CH-01",   // å±æ€§å:å±æ€§å€¼
      "chunk id": "6nphZ9wJ"
    }
  }
}
```

#### 7.3.4 å…³ç³»ç±»å‹åˆ†ç±»

æ ¹æ® Schema å®šä¹‰çš„å…³ç³»ç±»å‹ï¼š

**ç©ºé—´å…³ç³»**ï¼š
- `located_in`: ä½äº...ä½ç½®
- `part_of`: å±äº...çš„éƒ¨åˆ†
- `contains`: åŒ…å«...

**ç³»ç»Ÿå…³ç³»**ï¼š
- `belongs_to_system`: å±äºç³»ç»Ÿ
- `serves`: æœåŠ¡äº...

**äº§å“å…³ç³»**ï¼š
- `manufactured_by`: ç”±...åˆ¶é€ 
- `has_model`: å…·æœ‰å‹å·
- `installed_in`: å®‰è£…åœ¨...

**è¿æ¥å…³ç³»**ï¼š
- `connects_to`: è¿æ¥åˆ°...
- `controls`: æ§åˆ¶...
- `supplies`: ä¾›åº”...

### 7.4 èŠ‚ç‚¹å’Œè¾¹çš„åˆ›å»ºæœºåˆ¶

#### 7.4.1 èŠ‚ç‚¹IDç”Ÿæˆè§„åˆ™

**å”¯ä¸€é”®ç”Ÿæˆ**ï¼š
```python
# åŠ è½½æ—¶ä¸ºèŠ‚ç‚¹ç”Ÿæˆå”¯ä¸€é”®
start_key = (start_node_data["label"], start_name)
if start_key not in node_mapping:
    node_id = f"{start_node_data['label']}_{node_counter}"
    node_mapping[start_key] = node_id
    node_counter += 1
```

**å»é‡æœºåˆ¶**ï¼š
- **å®ä½“èŠ‚ç‚¹**ï¼šé€šè¿‡ `(label, name)` ç»„åˆå»é‡
- **å±æ€§èŠ‚ç‚¹**ï¼šé€šè¿‡å±æ€§å†…å®¹è‡ªåŠ¨å»é‡
- **ç›¸åŒå†…å®¹**ï¼šä¼šè¢«è¯†åˆ«ä¸ºåŒä¸€èŠ‚ç‚¹

#### 7.4.2 è¾¹åˆ›å»ºè§„åˆ™

**å…³ç³»æ ‡å‡†åŒ–**ï¼š
```python
# å…³ç³»ç±»å‹ä¸¥æ ¼æŒ‰ç…§Schemaå®šä¹‰
allowed_relations = [
    "located_in", "part_of", "belongs_to_system",
    "manufactured_by", "has_model", "installed_in",
    "serves", "connects_to", "controls", "supplies", "contains"
]

# åˆ›å»ºè¾¹æ—¶éªŒè¯å…³ç³»ç±»å‹
if relation not in allowed_relations:
    # è·³è¿‡æˆ–è®°å½•è­¦å‘Š
    continue
```

#### 7.4.3 èŠ‚ç‚¹å±‚æ¬¡è®¾ç½®

```python
# æ ¹æ®èŠ‚ç‚¹ç±»å‹è®¾ç½®å±‚æ¬¡çº§åˆ«
if start_node_data["label"] == "attribute":
    node_attrs["level"] = 1     # å±æ€§èŠ‚ç‚¹
elif start_node_data["label"] == "entity":
    node_attrs["level"] = 2     # å®ä½“èŠ‚ç‚¹
elif start_node_data["label"] == "keyword":
    node_attrs["level"] = 3     # å…³é”®è¯èŠ‚ç‚¹
elif start_node_data["label"] == "community":
    node_attrs["level"] = 4     # ç¤¾åŒºèŠ‚ç‚¹
```

### 7.5 èŠ‚ç‚¹å’Œè¾¹çš„å±æ€§è®¿é—®

#### 7.5.1 èŠ‚ç‚¹å±æ€§è·å–

```python
def _get_node_name(self, node_id: str) -> str:
    """è·å–èŠ‚ç‚¹çš„å¯è¯»åç§°"""
    node_data = self.graph.nodes.get(node_id, {})
    properties = node_data.get('properties', {})
    name = properties.get('name', node_id)

    # åˆ«åå¯¹é½å¤„ç†
    alias = properties.get('alias') or properties.get('aliases')
    if alias:
        return alias

    return name

def _get_node_properties(self, node: str) -> str:
    """è·å–èŠ‚ç‚¹çš„æ ¼å¼åŒ–å±æ€§"""
    data = self.graph.nodes[node]
    properties = []

    SKIP_FIELDS = {'name', 'description', 'properties', 'label', 'chunk id', 'level'}

    for key, value in data.get('properties', {}).items():
        if key not in SKIP_FIELDS:
            properties.append(f"{key}: {value}")

    return f"[{', '.join(properties)}]" if properties else ""
```

#### 7.5.2 è¾¹å±æ€§è·å–

```python
def get_edge_relation(self, u, v):
    """è·å–è¾¹çš„å…³ç³»ç±»å‹"""
    edge_data = self.graph.get_edge_data(u, v)
    if edge_data:
        # é¦–é€‰ relation å±æ€§
        relation = list(edge_data.values())[0].get('relation', '')
        if not relation:
            # å¤‡ç”¨ label å±æ€§
            relation = list(edge_data.values())[0].get('label', '')
        return relation
    return None

# ä½¿ç”¨ç¤ºä¾‹
for u, v, data in self.graph.edges(data=True):
    relation = data.get('relation', '') or data.get('label', '')
    if relation == 'located_in':
        # å¤„ç† located_in å…³ç³»
```

### 7.6 èŠ‚ç‚¹å’Œè¾¹çš„æ•°æ®éªŒè¯

#### 7.6.1 Schemaä¸€è‡´æ€§æ£€æŸ¥

```python
def validate_node_schema(self, node_id: str) -> bool:
    """éªŒè¯èŠ‚ç‚¹æ˜¯å¦ç¬¦åˆSchemaå®šä¹‰"""
    node_data = self.graph.nodes[node_id]
    properties = node_data.get('properties', {})

    schema_type = properties.get('schema_type', '')
    if schema_type not in self.schema['Nodes']:
        logger.warning(f"Unknown schema type: {schema_type}")
        return False

    return True

def validate_edge_relation(self, relation: str) -> bool:
    """éªŒè¯å…³ç³»ç±»å‹æ˜¯å¦ç¬¦åˆSchemaå®šä¹‰"""
    if relation not in self.schema['Relations']:
        logger.warning(f"Unknown relation type: {relation}")
        return False

    return True
```

#### 7.6.2 æ•°æ®å®Œæ•´æ€§æ£€æŸ¥

```python
def check_graph_integrity(self):
    """æ£€æŸ¥å›¾è°±æ•°æ®å®Œæ•´æ€§"""
    issues = []

    # æ£€æŸ¥èŠ‚ç‚¹å±æ€§å®Œæ•´æ€§
    for node_id, node_data in self.graph.nodes(data=True):
        properties = node_data.get('properties', {})

        if not properties.get('name'):
            issues.append(f"Node {node_id} missing name")

        if not properties.get('chunk id'):
            issues.append(f"Node {node_id} missing chunk id")

    # æ£€æŸ¥è¾¹å…³ç³»æœ‰æ•ˆæ€§
    for u, v, data in self.graph.edges(data=True):
        relation = data.get('relation', '')
        if not relation:
            issues.append(f"Edge ({u}, {v}) missing relation")

    return issues
```

### 7.7 èŠ‚ç‚¹å’Œè¾¹çš„æ•°æ®æ¼”è¿›

#### 7.7.1 èŠ‚ç‚¹å±æ€§æ‰©å±•

éšç€ç³»ç»Ÿä½¿ç”¨ï¼ŒèŠ‚ç‚¹å±æ€§ä¼šé€æ¸ä¸°å¯Œï¼š

```json
// åˆå§‹åˆ›å»º
{
  "name": "Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº",
  "schema_type": "asset",
  "chunk id": "6nphZ9wJ"
}

// ä½¿ç”¨è¿‡ç¨‹ä¸­æ·»åŠ çš„å±æ€§
{
  "name": "Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº",
  "schema_type": "asset",
  "chunk id": "6nphZ9wJ",
  "alias": ["Aæ ‹å†·æœº1å·", "1å·å†·æœº"],           // åˆ«å
  "description": "ç¦»å¿ƒå¼å†·æœºï¼Œåˆ¶å†·é‡1000kW",    // æè¿°
  "install_date": "2022-03-15",                 // å®‰è£…æ—¥æœŸ
  "maintenance_history": ["2023-01-01", "2023-07-01"] // ç»´æŠ¤å†å²
}
```

#### 7.7.2 å…³ç³»ç½‘ç»œæ‰©å±•

å›¾è°±ä¼šéšç€çŸ¥è¯†çš„ç§¯ç´¯å½¢æˆæ›´ä¸°å¯Œçš„ç½‘ç»œç»“æ„ï¼š

```
åˆå§‹çŠ¶æ€:
Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº --located_in--> LOC-A-B1-MECH
LOC-A-B1-MECH --part_of--> Aæ ‹B1å±‚
Aæ ‹B1å±‚ --part_of--> Aæ ‹

æ¼”è¿›çŠ¶æ€:
Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº --located_in--> LOC-A-B1-MECH
Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº --belongs_to_system--> HVACç³»ç»Ÿ
Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº --manufactured_by--> Johnson Controls
Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº --has_model--> YVAA-C2-03
Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº --has_attribute--> asset_id: A-CH-01
Aæ ‹1å·ç¦»å¿ƒå¼å†·æœº --has_attribute--> install_date: 2022-03-15
LOC-A-B1-MECH --part_of--> Aæ ‹B1å±‚
LOC-A-B1-MECH --serves--> ç©ºè°ƒç³»ç»Ÿ
Aæ ‹B1å±‚ --part_of--> Aæ ‹
Aæ ‹B1å±‚ --contains--> å¤šä¸ªä½ç½®
HVACç³»ç»Ÿ --contains--> å¤šä¸ªå†·æœº
Johnson Controls --manufactures--> å¤šä¸ªè®¾å¤‡
```

### 7.8 æ€»ç»“

èŠ‚ç‚¹å’Œè¾¹çš„æ•°æ®ç»“æ„è®¾è®¡ä½“ç°äº†ä»¥ä¸‹åŸåˆ™ï¼š

1. **å±‚æ¬¡åŒ–ç»„ç»‡**ï¼šå››å±‚èŠ‚ç‚¹ç»“æ„ï¼ˆå±æ€§â†’å®ä½“â†’å…³é”®è¯â†’ç¤¾åŒºï¼‰
2. **å±æ€§å®Œæ•´æ€§**ï¼šæ¯ä¸ªèŠ‚ç‚¹éƒ½åŒ…å«å®Œæ•´çš„å±æ€§ä¿¡æ¯
3. **å…³ç³»è§„èŒƒåŒ–**ï¼šä¸¥æ ¼æŒ‰ç…§Schemaå®šä¹‰çš„å…³ç³»ç±»å‹
4. **è‡ªæè¿°æ€§**ï¼šèŠ‚ç‚¹å’Œè¾¹éƒ½åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯è¿›è¡Œç†è§£
5. **æ‰©å±•æ€§**ï¼šæ”¯æŒåŠ¨æ€æ·»åŠ æ–°å±æ€§å’Œå…³ç³»ç±»å‹

è¿™ç§è®¾è®¡ç¡®ä¿äº†å›¾è°±æ•°æ®çš„ç»“æ„åŒ–ã€æ ‡å‡†åŒ–å’Œå¯æ‰©å±•æ€§ï¼Œä¸ºå¤šè·¯æ··åˆæ£€ç´¢æä¾›äº†åšå®çš„æ•°æ®åŸºç¡€ã€‚

## 8. æ£€ç´¢ç»“æœèåˆ

### 8.1 èåˆç­–ç•¥

ç³»ç»Ÿé‡‡ç”¨å¤šç­–ç•¥èåˆæœºåˆ¶ï¼Œæ ¹æ®æŸ¥è¯¢ç‰¹å¾å’Œè·¯å¾„ç‰¹æ€§è¿›è¡Œç»“æœåˆå¹¶ï¼š

1. **è·¯å¾„1ç»“æœ**ï¼šå›¾è°±éå†æ‰¾åˆ°çš„chunk_ids
2. **è·¯å¾„2ç»“æœ**ï¼šä¸‰å…ƒç»„æ£€ç´¢æ‰¾åˆ°çš„chunk_ids
3. **è·¯å¾„3ç»“æœ**ï¼šè¯­ä¹‰æ£€ç´¢æ‰¾åˆ°çš„chunk_ids

### 8.2 æƒé‡åˆ†é…

```python
# ä¸åŒè·¯å¾„çš„æƒé‡åˆ†é…
PATH_WEIGHTS = {
    'path1': 0.95,      # å›¾è°±éå†ï¼šæœ€é«˜æƒé‡
    'path2': 0.85,      # ä¸‰å…ƒç»„æ£€ç´¢ï¼šä¸­ç­‰æƒé‡
    'path3': 0.75       # è¯­ä¹‰æ£€ç´¢ï¼šåŸºç¡€æƒé‡
}

def merge_results(self, path1_results, path2_results, path3_results):
    """èåˆå¤šè·¯å¾„ç»“æœ"""
    all_results = {}

    # è·¯å¾„1ç»“æœï¼šé«˜æƒé‡
    for chunk_id in path1_results:
        all_results[chunk_id] = PATH_WEIGHTS['path1']

    # è·¯å¾„2ç»“æœï¼šä¸­ç­‰æƒé‡
    for chunk_id in path2_results:
        if chunk_id in all_results:
            all_results[chunk_id] = max(all_results[chunk_id], PATH_WEIGHTS['path2'])
        else:
            all_results[chunk_id] = PATH_WEIGHTS['path2']

    # è·¯å¾„3ç»“æœï¼šåŸºç¡€æƒé‡
    for chunk_id in path3_results:
        if chunk_id in all_results:
            all_results[chunk_id] = max(all_results[chunk_id], PATH_WEIGHTS['path3'])
        else:
            all_results[chunk_id] = PATH_WEIGHTS['path3']

    # æ’åºå¹¶è¿”å›
    sorted_results = sorted(all_results.items(), key=lambda x: x[1], reverse=True)
    return [chunk_id for chunk_id, score in sorted_results]
```

## 9. å®é™…è¿è¡Œç¤ºä¾‹

### 9.1 æŸ¥è¯¢ï¼š"Aæ ‹3Fæœ‰å“ªäº›è®¾å¤‡"

#### è·¯å¾„1ç»“æœï¼ˆå›¾è°±éå†ï¼‰ï¼š
```
- Chunk ID: YZ1N7DbR (0.95) - Aæ ‹3å±‚ç©ºè°ƒç®±
- Chunk ID: NcV6s6oz (0.94) - Aæ ‹ä¸‰å±‚æ•å¼€åŠå…¬åŒº
```

#### è·¯å¾„2ç»“æœï¼ˆä¸‰å…ƒç»„æ£€ç´¢ï¼‰ï¼š
```
- Chunk ID: p_35TCOR (0.85) - B1æ¶ˆé˜²æ°´æ³µæˆ¿
- Chunk ID: DVlR0QrN (0.84) - Aæ ‹6å±‚ç©ºè°ƒç®±
```

#### è·¯å¾„3ç»“æœï¼ˆè¯­ä¹‰æ£€ç´¢ï¼‰ï¼š
```
- Chunk ID: YZ1N7DbR (0.75) - Aæ ‹3å±‚ç©ºè°ƒç®±
- Chunk ID: z-4wP8MI (0.74) - Aæ ‹ä¸‰å±‚åŠå…¬åŒº
```

#### èåˆç»“æœï¼š
```
- YZ1N7DbR (0.95) - å›¾è°±ç²¾ç¡®åŒ¹é…
- NcV6s6oz (0.94) - å›¾è°±æ‰©å±•
- p_35TCOR (0.85) - ä¸‰å…ƒç»„ç›¸å…³
- DVlR0QrN (0.84) - ä¸‰å…ƒç»„æ‰©å±•
- z-4wP8MI (0.75) - è¯­ä¹‰è¡¥å……
```

### 9.2 æ€§èƒ½å¯¹æ¯”

| è·¯å¾„ | å“åº”æ—¶é—´ | å‡†ç¡®ç‡ | å¬å›ç‡ |
|------|----------|--------|--------|
| è·¯å¾„1 | 50ms | 95% | 70% |
| è·¯å¾„2 | 200ms | 85% | 80% |
| è·¯å¾„3 | 150ms | 75% | 90% |
| èåˆ | 220ms | 98% | 95% |

## 10. æ ¸å¿ƒä¼˜åŠ¿åˆ†æ

### 10.1 å¤šè·¯å¾„äº’è¡¥

1. **ç»“æ„åŒ– vs è¯­ä¹‰åŒ–**ï¼š
   - è·¯å¾„1ï¼šç»“æ„åŒ–æŸ¥è¯¢çš„ç²¾ç¡®åŒ¹é…
   - è·¯å¾„2/3ï¼šæ¨¡ç³ŠæŸ¥è¯¢çš„è¯­ä¹‰ç†è§£

2. **è§„åˆ™ vs å­¦ä¹ **ï¼š
   - è·¯å¾„1ï¼šåŸºäºè§„åˆ™çš„ç¡®å®šæ€§æ¨ç†
   - è·¯å¾„2/3ï¼šåŸºäºå­¦ä¹ çš„æ¦‚ç‡æ€§æ¨ç†

3. **å¿«é€Ÿ vs å…¨é¢**ï¼š
   - è·¯å¾„1ï¼šå¿«é€Ÿä½†å¯èƒ½é—æ¼
   - è·¯å¾„2/3ï¼šå…¨é¢ä½†è®¡ç®—å¯†é›†

### 10.2 åŠ¨æ€ç´¢å¼•ä¼˜åŠ¿

- **å†…å­˜æ•ˆç‡**ï¼šé¿å…å¤§å‹ç´¢å¼•æ–‡ä»¶çš„ç£ç›˜å ç”¨
- **æ›´æ–°çµæ´»**ï¼šå¯ä»¥æ ¹æ®æœ€æ–°æ•°æ®é‡å»º
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå†…å­˜æœç´¢æ¯”ç£ç›˜æœç´¢å¿«æ•°å€
- **å®ç°ç®€å•**ï¼šæ— éœ€å¤æ‚çš„ç´¢å¼•ç®¡ç†é€»è¾‘

### 10.3 èåˆç­–ç•¥ä¼˜åŒ–

- **æƒé‡ç§‘å­¦**ï¼šæ ¹æ®è·¯å¾„ç‰¹æ€§åˆ†é…ä¸åŒæƒé‡
- **é˜ˆå€¼æ§åˆ¶**ï¼šé¿å…ä½è´¨é‡ç»“æœå¹²æ‰°
- **å»é‡æœºåˆ¶**ï¼šç¡®ä¿ç»“æœçš„å”¯ä¸€æ€§å’Œå¤šæ ·æ€§
- **æ’åºä¼˜åŒ–**ï¼šç»¼åˆè€ƒè™‘ç›¸å…³æ€§å’Œå¤šæ ·æ€§

---

*æœ¬æ–‡æ¡£åŸºäºYoutuGraphRAGé¡¹ç›®çš„å®é™…ä»£ç åˆ†æå’Œè¿è¡Œæµ‹è¯•æ•´ç†è€Œæˆï¼Œè¯¦ç»†é˜è¿°äº†å¤šè·¯æ··åˆæ£€ç´¢ç³»ç»Ÿçš„è®¾è®¡ç†å¿µã€å®ç°æœºåˆ¶å’Œæ€§èƒ½ä¼˜åŠ¿ã€‚*
